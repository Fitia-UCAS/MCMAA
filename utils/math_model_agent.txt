ç›®å½•ç»“æ„:
|-- æ–‡ä»¶: .env.dev
|-- æ–‡ä»¶: .env.dev.example
|-- æ–‡ä»¶: .gitignore
|-- æ–‡ä»¶å¤¹: app
|   |-- æ–‡ä»¶å¤¹: config
|   |   |-- æ–‡ä»¶: md_template.toml
|   |   |-- æ–‡ä»¶: model_config.toml
|   |   |-- æ–‡ä»¶: setting.py
|   |   |-- æ–‡ä»¶: template.md
|   |-- æ–‡ä»¶å¤¹: core
|   |   |-- æ–‡ä»¶å¤¹: agents
|   |   |   |-- æ–‡ä»¶: agent.py
|   |   |   |-- æ–‡ä»¶: coder_agent.py
|   |   |   |-- æ–‡ä»¶: coordinator_agent.py
|   |   |   |-- æ–‡ä»¶: modeler_agent.py
|   |   |   |-- æ–‡ä»¶: writer_agent.py
|   |   |   |-- æ–‡ä»¶: __init__.py
|   |   |-- æ–‡ä»¶: flows.py
|   |   |-- æ–‡ä»¶: functions.py
|   |   |-- æ–‡ä»¶å¤¹: llm
|   |   |   |-- æ–‡ä»¶: llm.py
|   |   |   |-- æ–‡ä»¶: llm_factory.py
|   |   |   |-- æ–‡ä»¶: __init__.py
|   |   |-- æ–‡ä»¶: prompts.py
|   |   |-- æ–‡ä»¶: workflow.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: example
|   |   |-- æ–‡ä»¶å¤¹: example
|   |   |   |-- æ–‡ä»¶å¤¹: 2023åæ•°æ¯Cé¢˜
|   |   |   |   |-- æ–‡ä»¶: questions.txt
|   |   |   |   |-- æ–‡ä»¶: åæ•°æ¯2023å¹´Cé¢˜.pdf
|   |   |   |   |-- æ–‡ä»¶: é™„ä»¶.xlsx
|   |   |   |-- æ–‡ä»¶å¤¹: 2024é«˜æ•™æ¯Cé¢˜
|   |   |   |   |-- æ–‡ä»¶: Cé¢˜.pdf
|   |   |   |   |-- æ–‡ä»¶: questions.txt
|   |   |   |   |-- æ–‡ä»¶: result1_1.xlsx
|   |   |   |   |-- æ–‡ä»¶: result1_2.xlsx
|   |   |   |   |-- æ–‡ä»¶: result2.xlsx
|   |   |   |   |-- æ–‡ä»¶: é™„ä»¶1.xlsx
|   |   |   |   |-- æ–‡ä»¶: é™„ä»¶2.xlsx
|   |   |   |-- æ–‡ä»¶å¤¹: 2025äº”ä¸€æ¯Cé¢˜
|   |   |   |   |-- æ–‡ä»¶: 2025-51MCM-Problem C.docx
|   |   |   |   |-- æ–‡ä»¶: questions.txt
|   |   |   |   |-- æ–‡ä»¶: é™„ä»¶1 (Attachment 1).csv
|   |   |   |   |-- æ–‡ä»¶: é™„ä»¶2 (Attachment 2).csv
|   |   |-- æ–‡ä»¶å¤¹: sample_data
|   |   |   |-- æ–‡ä»¶: é™„ä»¶.xlsx
|   |-- æ–‡ä»¶: main.py
|   |-- æ–‡ä»¶å¤¹: models
|   |   |-- æ–‡ä»¶: user_output.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: routers
|   |   |-- æ–‡ä»¶: common_router.py
|   |   |-- æ–‡ä»¶: files_router.py
|   |   |-- æ–‡ä»¶: modeling_router.py
|   |   |-- æ–‡ä»¶: ws_router.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: schemas
|   |   |-- æ–‡ä»¶: A2A.py
|   |   |-- æ–‡ä»¶: base.py
|   |   |-- æ–‡ä»¶: enums.py
|   |   |-- æ–‡ä»¶: request.py
|   |   |-- æ–‡ä»¶: response.py
|   |   |-- æ–‡ä»¶: tool_result.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: services
|   |   |-- æ–‡ä»¶: redis_manager.py
|   |   |-- æ–‡ä»¶: ws_manager.py
|   |-- æ–‡ä»¶å¤¹: tests
|   |   |-- æ–‡ä»¶: get_config_template.py
|   |   |-- æ–‡ä»¶å¤¹: mock
|   |   |   |-- æ–‡ä»¶: res.json
|   |   |-- æ–‡ä»¶: res.md
|   |   |-- æ–‡ä»¶: test_common_utils.py
|   |   |-- æ–‡ä»¶: test_e2b.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: tools
|   |   |-- æ–‡ä»¶: base.py
|   |   |-- æ–‡ä»¶: base_interpreter.py
|   |   |-- æ–‡ä»¶: e2b_interpreter.py
|   |   |-- æ–‡ä»¶: interpreter_factory.py
|   |   |-- æ–‡ä»¶: local_interpreter.py
|   |   |-- æ–‡ä»¶: notebook_serializer.py
|   |   |-- æ–‡ä»¶: openalex_scholar.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶å¤¹: utils
|   |   |-- æ–‡ä»¶: cli.py
|   |   |-- æ–‡ä»¶: common_utils.py
|   |   |-- æ–‡ä»¶: data_recorder.py
|   |   |-- æ–‡ä»¶: log_util.py
|   |   |-- æ–‡ä»¶: RichPrinter.py
|   |   |-- æ–‡ä»¶: track.py
|   |   |-- æ–‡ä»¶: __init__.py
|   |-- æ–‡ä»¶: __init__.py
|-- æ–‡ä»¶å¤¹: logs
|   |-- æ–‡ä»¶å¤¹: messages
|   |   |-- (ç©ºç›®å½•)
|-- æ–‡ä»¶å¤¹: project
|   |-- æ–‡ä»¶å¤¹: work_dir
|   |   |-- æ–‡ä»¶: .gitkeep
|-- æ–‡ä»¶: requirements.txt
|-- æ–‡ä»¶: start


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\.env.dev çš„å†…å®¹:
================================================================================
ENV=dev
# support all model, check out https://docs.litellm.ai/docs/providers
# e.g. gpt-4.1,deepseek/deepseek-chat
# docs/md/tutorial
# ä¸ºæ¯ä¸ª agent é…ç½®åˆé€‚çš„æ¨¡å‹
COORDINATOR_API_KEY=sk-2f4189e3911644c6a37c28a202e0295c
COORDINATOR_MODEL=deepseek/deepseek-chat
# COORDINATOR_BASE_URL= å¦‚æœéœ€è¦ä¸­è½¬è‡ªå®šä¹‰ç­‰ åœ°å€åé¢æ·»åŠ  /v1

# æ¨è thinking model
MODELER_API_KEY=sk-2f4189e3911644c6a37c28a202e0295c
MODELER_MODEL=deepseek/deepseek-chat
# MODELER_BASE_URL=

CODER_API_KEY=sk-2f4189e3911644c6a37c28a202e0295c
CODER_MODEL=deepseek/deepseek-chat
# CODER_BASE_URL=

WRITER_API_KEY=sk-2f4189e3911644c6a37c28a202e0295c
WRITER_MODEL=deepseek/deepseek-chat
# WRITER_BASE_URL=

DEFAULT_API_KEY=sk-2f4189e3911644c6a37c28a202e0295c
DEFAULT_MODEL=deepseek/deepseek-chat
# DEFAULT_BASE_URL= 

# æ¨¡å‹æœ€å¤§é—®ç­”æ¬¡æ•°
MAX_CHAT_TURNS=60
# æ€è€ƒåæ€æ¬¡æ•°
MAX_RETRIES=5

# ä¸éœ€è¦å¡«ï¼Œé»˜è®¤è°ƒç”¨æœ¬åœ° Python
# E2B_API_KEY=
SERVER_HOST=http://localhost:8000
# ä½¿ç”¨ email æ³¨å†Œè´¦å·ä» https://openalex.org/ æ–‡çŒ®
OPENALEX_EMAIL=2039787966@qq.com

LOG_LEVEL=DEBUG
DEBUG=true
# ç¡®ä¿å®‰è£… Redis
# å¦‚æœæ˜¯docker: REDIS_URL=redis://redis:6379/0
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=20
CORS_ALLOW_ORIGINS=http://localhost:5173,http://localhost:3000

================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\.env.dev.example çš„å†…å®¹:
================================================================================
ENV=dev
# support all model, check out https://docs.litellm.ai/docs/providers
# e.g. gpt-4.1,deepseek/deepseek-chat
# docs/md/tutorial
# ä¸ºæ¯ä¸ª agent é…ç½®åˆé€‚çš„æ¨¡å‹
COORDINATOR_API_KEY=
COORDINATOR_MODEL=
# COORDINATOR_BASE_URL= å¦‚æœéœ€è¦ä¸­è½¬è‡ªå®šä¹‰ç­‰ åœ°å€åé¢æ·»åŠ  /v1

# æ¨è thinking model
MODELER_API_KEY=
MODELER_MODEL=
# MODELER_BASE_URL=

CODER_API_KEY=
CODER_MODEL=
# CODER_BASE_URL=

WRITER_API_KEY=
WRITER_MODEL=
# WRITER_BASE_URL=

DEFAULT_API_KEY=
DEFAULT_MODEL=
# DEFAULT_BASE_URL= 

# æ¨¡å‹æœ€å¤§é—®ç­”æ¬¡æ•°
MAX_CHAT_TURNS=60
# æ€è€ƒåæ€æ¬¡æ•°
MAX_RETRIES=5

# ä¸éœ€è¦å¡«ï¼Œé»˜è®¤è°ƒç”¨æœ¬åœ° Python
# E2B_API_KEY=
SERVER_HOST=http://localhost:8000
# ä½¿ç”¨ email æ³¨å†Œè´¦å·ä» https://openalex.org/ æ–‡çŒ®
OPENALEX_EMAIL=

LOG_LEVEL=DEBUG
DEBUG=true
# ç¡®ä¿å®‰è£… Redis
# å¦‚æœæ˜¯docker: REDIS_URL=redis://redis:6379/0
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=20
CORS_ALLOW_ORIGINS=http://localhost:5173,http://localhost:3000

================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\start çš„å†…å®¹:
================================================================================
conda activate mcmaa_tk
cd E:\repo1\MCM\mcmaa\math_model_agent
python -m app.main
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\main.py çš„å†…å®¹:
================================================================================
from fastapi import FastAPI
from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
import os
from app.routers import modeling_router, ws_router, common_router, files_router
from app.utils.log_util import logger
from app.config.setting import settings
from fastapi.staticfiles import StaticFiles
from app.utils.cli import get_ascii_banner, center_cli_str


@asynccontextmanager
async def lifespan(app: FastAPI):
    print(get_ascii_banner())
    print(center_cli_str("GitHub:https://github.com/jihe520/MathModelAgent"))
    logger.info("Starting MathModelAgent")

    PROJECT_FOLDER = "./project"
    os.makedirs(PROJECT_FOLDER, exist_ok=True)

    yield
    logger.info("Stopping MathModelAgent")


app = FastAPI(
    title="MathModelAgent",
    description="Agents for MathModel",
    version="0.1.0",
    lifespan=lifespan,
)

app.include_router(modeling_router.router)
app.include_router(ws_router.router)
app.include_router(common_router.router)
app.include_router(files_router.router)


# è·¨åŸŸ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ALLOW_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],  # æš´éœ²æ‰€æœ‰å“åº”å¤´
)

app.mount(
    "/static",  # è¿™æ˜¯è®¿é—®æ—¶çš„å‰ç¼€
    StaticFiles(directory="project/work_dir"),  # è¿™æ˜¯æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„
    name="static",
)


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\config\md_template.toml çš„å†…å®¹:
================================================================================
firstPage = """
# æ ‡é¢˜ï¼š åŸºäºå…¨æ–‡å–ä¸€ä¸ªæ ‡é¢˜
ä¾‹å­ï¼šåŸºäºå“ˆé‡Œæ–¯é¹°ç®—æ³•çš„å†œä¸šç§æ¤ä¼˜åŒ–æ¨¡å‹ç ”ç©¶
æ‘˜è¦
ç¬¬ä¸€æ®µï¼šèƒŒæ™¯
å¤§çº¦100å­—

æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦æŒ‰ç…§ä¸‹é¢ç±»ä¼¼çš„å•ç‹¬æ’°å†™æˆä¸€æ®µ'\n'ï¼Œä¸è¦åˆ†ç‚¹å™è¿°ã€‚
é’ˆå¯¹é—®é¢˜å‡ ï¼Œæ ¹æ®é—®é¢˜å‡ å›ç­”ï¼Œä½¿ç”¨åˆ°ä»€ä¹ˆæ¨¡å‹ï¼Œå¦‚ä½•å»æ„å»ºè¯¥æ¨¡å‹ï¼Œåšäº†ä»€ä¹ˆå·¥ä½œï¼Œå¾—åˆ°ä»€ä¹ˆç»“æœï¼Œä¸€äº›å…·ä½“çš„æ•°å€¼
æ¯æ®µå¤§çº¦170å­—

æœ€åä¸€æ®µï¼šæ€»ç»“æ•æ„Ÿæ€§åˆ†æä½¿ç”¨å’Œæ•ˆæœï¼Œç»“è®ºã€‚
å¤§çº¦150å­—

å…³é”®è¯ï¼š ç”¨åˆ°çš„æ¨¡å‹ç­‰ï¼Œå…³é”®è¯å¤§çº¦4/5ä¸ªï¼Œæ²¡æœ‰æ ‡å·
ä¾‹å­ï¼šHHO  MOHHO  é—ä¼ ç®—æ³•  Mealpy  è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ  è§„åˆ’é—®é¢˜

å‚è€ƒä¸‹é¢çš„é—®é¢˜,æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£ï¼ŒæŒ‰ç…§ä¸Šé¢æ¨¡æ¿å’Œè¦æ±‚å®Œæˆ æ ‡é¢˜ï¼Œæ‘˜è¦ï¼Œå…³é”®å­—çš„æ’°å†™

{é—®é¢˜}
{æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£}
"""
RepeatQues = """
# ä¸€ã€é—®é¢˜é‡è¿°
## 1.1 é—®é¢˜èƒŒæ™¯
æŸ¥è¯¢æ–‡çŒ®
æ ¹æ®é¢˜ç›®æ€»ç»“é—®é¢˜èƒŒæ™¯ï¼ˆæŸ¥è¯¢æ–‡çŒ®ï¼‰
æ ¼å¼ï¼šä¸€æ®µè¯
å¤§çº¦160å­—
## 1.2 é—®é¢˜é‡è¿°
å‚è€ƒæ¨¡æ¿å¦‚ä¸‹
æœ¬æ–‡åŸºäºä»¥ä¸Šä¿¡æ¯å»ºç«‹æ•°å­¦æ¨¡å‹æ¥è§£å†³ä»¥ä¸‹é—®é¢˜ã€‚
åˆ†ç‚¹å™è¿°

**å‚è€ƒä¸‹é¢çš„é¢˜ç›®è¦æ±‚å’Œé—®é¢˜ï¼ŒæŒ‰ç…§ä¸Šé¢æ¨¡æ¿å’Œè¦æ±‚å®Œæˆ **é—®é¢˜é‡è¿°ä¸­çš„é—®é¢˜èƒŒæ™¯å’Œé—®é¢˜é‡è¿°**

{é¢˜ç›®}
"""
analysisQues = """
# äºŒã€é—®é¢˜åˆ†æ
## 2.1 é—®é¢˜ä¸€çš„åˆ†æ
åˆ†ä¸¤æ®µ
å¤§çº¦250å­—

å‚è€ƒæ¨¡æ¿ï¼š
æœ¬é¢˜è¦æ±‚æ ¹æ®é”€å”®é‡ã€ç§æ¤æˆæœ¬ã€äº©äº§é‡å’Œé”€å”®ä»·æ ¼ç­‰ç›¸å…³æ•°æ®ï¼Œè€ƒè™‘åœŸåœ°èµ„æºã€ç§æ¤æ¡ä»¶ç­‰çº¦æŸæ¡ä»¶ï¼Œå°†é—®é¢˜ä¸€æ‹†è§£æˆä¸¤å°é—®ï¼Œåœ¨ä¸¤ç§ä¸åŒçš„æ»é”€å¤„ç†æ–¹å¼å³ä¸¤ç§ä¸åŒçš„å•ç›®æ ‡å‡½æ•°ä¸‹æ±‚è§£å‡º2024-2030çš„åˆ©æ¶¦æœ€ä¼˜ç§æ¤æ–¹æ¡ˆã€‚
å‡è®¾2023å¹´çš„å†œä½œç‰©æ•°æ®ï¼ˆé”€å”®é‡ã€ä»·æ ¼ã€äº©äº§é‡ã€ç§æ¤æˆæœ¬ï¼‰ä¿æŒä¸å˜ï¼Œå¹¶ä¸è€ƒè™‘ç¯å¢ƒçš„æ³¢åŠ¨æ€§ï¼Œå¯ä»¥é€šè¿‡å»ºç«‹å•ä¸€ç›®æ ‡çš„è§„åˆ’æ¨¡å‹è¿›è¡Œæ±‚è§£ã€‚é‡‡ç”¨å“ˆé‡Œæ–¯é¹°ä¼˜åŒ–ç®—æ³•æ¨¡å‹(HHO) æ±‚è§£æœ€ä¼˜ä½œç‰©ç§æ¤æ–¹å¼ï¼Œä¸é—ä¼ ç®—æ³•ï¼ˆGAï¼‰å¯¹æ¯”ï¼Œå¹¶å¯¹æ¨¡å‹çš„è§„åˆ’ç»“æœè¿›è¡Œè¯„ä¼°ï¼Œå“ˆé‡Œæ–¯é¹°ä¼˜åŒ–ç®—æ³•æ‹¥æœ‰æ›´å¼ºçš„å±€éƒ¨æœç´¢èƒ½åŠ›å’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼Œåœ¨é—®é¢˜çš„è§£å†³ä¸Šæ›´åŠ çµæ´»å’Œæ•æ·ï¼Œèƒ½å¤Ÿæ›´åŠ å¿«é€Ÿçš„æœæŸ¥å’Œæ±‡æ€»æ•°æ®ï¼Œæœ€ç»ˆé€‰æ‹©å“ˆé‡Œæ–¯é¹°ç®—æ³•æ¨¡å‹(HHO)è¿›è¡Œæ±‚è§£ã€‚

æ¯ä¸ªé—®é¢˜å¦‚æ­¤åˆ†æ
## 2.2 é—®é¢˜äºŒçš„åˆ†æ


å‚è€ƒä¸‹é¢çš„æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£ å’Œ é¢˜ç›® ï¼ŒæŒ‰ç…§ä¸Šé¢æ¨¡æ¿å’Œè¦æ±‚å®Œæˆ **æ¯ä¸ªé—®é¢˜çš„åˆ†æ**

{æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£}
{é¢˜ç›®}
  """
modelAssumption = """
# ä¸‰ã€æ¨¡å‹å‡è®¾
å†™å¤§æ¦‚3/4æ¡ï¼Œä¸‹é¢ä¾‹å­ï¼š
(1) æ•°æ®æœ‰æ•ˆæ€§ï¼šå‡è®¾æ‰€ç”¨æ•°æ®çœŸå®å¯é 
(2) å¸‚åœºç¨³å®šæ€§ï¼šå‡è®¾å†œä¸šç§æ¤ä¸­æ²¡æœ‰è‡ªç„¶ç¾å®³ã€æ”¿åºœæ”¿ç­–ç­‰å› ç´ å½±å“ã€‚

å‚è€ƒä¸‹é¢çš„æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£ å’Œ é¢˜ç›® ï¼ŒæŒ‰ç…§ä¸Šé¢æ¨¡æ¿å’Œè¦æ±‚å®Œæˆ **æ¨¡å‹çš„å‡è®¾**

{æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£}
{é¢˜ç›®}
  """
symbol = """
# å››ã€ç¬¦å·è¯´æ˜å’Œæ•°æ®é¢„å¤„ç†
## 4.1 ç¬¦å·è¯´æ˜

å‚è€ƒæ¨¡æ¿ï¼š
| ç¬¦å· | å«ä¹‰ | å•ä½ |
| ---- | ---- | ---- |
| \\( x \\) | å†œä½œç‰©ç§ç±» | - |
| \\( y \\) | ç§æ¤é¢ç§¯ | äº© |
| \\( z \\) | é¢„æœŸé”€å”®é‡ | å¨ |
| \\( c \\) | ç§æ¤æˆæœ¬ | å…ƒ |
| \\( p \\) | é”€å”®ä»·æ ¼ | å…ƒ/å¨ |
| \\( r \\) | é£é™©ç³»æ•° | - |
| \\( t \\) | æ—¶é—´ï¼ˆå¹´ï¼‰ | å¹´ |

å‚è€ƒä¸‹é¢çš„æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£ï¼Œç»™å‡ºè¡¨æ ¼è¡¨ç¤ºä¸€äº›ç¬¦å·è¯´æ˜
æ³¨æ„ï¼šç¬¦å·ç”¨markdwonå…¬å¼æ¸²æŸ“

{æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£}
"""
eda = """
## 4.2 æè¿°æ€§ç»Ÿè®¡
å¤§çº¦200å­—
"""
ques1 = """æ¨¡æ¿å’Œè¦æ±‚å¦‚ä¸‹
# äº”ã€æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
## 5.1 é—®é¢˜ä¸€æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.1.1 é—®é¢˜çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.1.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
ques2 = """å‚è€ƒæ¨¡æ¿
## 5.2 é—®é¢˜äºŒæ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.2.1 é—®é¢˜çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.2.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
ques3 = """å‚è€ƒæ¨¡æ¿
## 5.3 é—®é¢˜ä¸‰æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.3.1 æ¨¡å‹çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.3.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
ques4 = """å‚è€ƒæ¨¡æ¿
## 5.4 é—®é¢˜å››æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.4.1 æ¨¡å‹çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.4.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
ques5 = """å‚è€ƒæ¨¡æ¿
## 5.5 é—®é¢˜äº”æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.5.1 æ¨¡å‹çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.5.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
ques6 = """å‚è€ƒæ¨¡æ¿
## 5.6 é—®é¢˜å…­æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£
### 5.6.1 æ¨¡å‹çš„å»ºç«‹
å¯¹æ¨¡å‹çš„ç®€å•ä»‹ç»ï¼Œç”¨åˆ°çš„å…¬å¼ï¼Œæ¨¡å‹çš„å»ºç«‹è¿‡ç¨‹
### 5.6.2 æ¨¡å‹çš„æ±‚è§£
æ¨¡å‹çš„éªŒè¯ï¼Œç»“æœåˆ†æï¼Œå›ç­”é—®é¢˜
æ¨¡å‹çš„æ±‚è§£è¿‡ç¨‹
å¤§çº¦600å­—
"""
sensitivity_analysis = """å‚è€ƒæ¨¡æ¿
# å…­ã€æ¨¡å‹çš„åˆ†æä¸æ£€éªŒ
## 6.1 çµæ•åº¦åˆ†æ
"""
judge = """å‚è€ƒæ¨¡æ¿å’Œè¦æ±‚
# ä¸ƒã€æ¨¡å‹çš„è¯„ä»·ã€æ”¹è¿›ä¸æ¨å¹¿
## 7.1 æ¨¡å‹çš„ä¼˜ç‚¹
## 7.2 æ¨¡å‹çš„ç¼ºç‚¹
## 7.3 æ¨¡å‹çš„æ”¹è¿›ä¸æ¨å¹¿
ä¼˜ç‚¹æ•°é‡è¦å¤šäºç¼ºç‚¹ï¼Œç¼ºç‚¹å¤§çº¦2/3ä¸ª
å¤§çº¦200å­—
"""

================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\config\model_config.toml çš„å†…å®¹:
================================================================================
[config1]
COORDINATOR_API_KEY=''
COORDINATOR_MODEL=''
COORDINATOR_BASE_URL=''


MODELER_API_KEY=''
MODELER_MODEL=''
MODELER_BASE_URL=''

CODER_API_KEY=''
CODER_MODEL=''
CODER_BASE_URL=''

WRITER_API_KEY=''
WRITER_MODEL=''
WRITER_BASE_URL=''

[config2]


[current]
current = 'config1'


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\config\setting.py çš„å†…å®¹:
================================================================================
from pydantic import AnyUrl, BeforeValidator, computed_field, field_validator, Field
from pydantic_settings import BaseSettings, SettingsConfigDict
import os
from typing import Annotated, Optional


def parse_cors(value: str) -> list[str]:
    """
    Parses the CORS settings from a string to a list of URLs.
    """
    if value == "*":
        return ["*"]
    if "," in value:
        return [url.strip() for url in value.split(",")]
    return [value]


class Settings(BaseSettings):
    ENV: str

    COORDINATOR_API_KEY: str
    COORDINATOR_MODEL: str
    COORDINATOR_BASE_URL: Optional[str] = None

    MODELER_API_KEY: str
    MODELER_MODEL: str
    MODELER_BASE_URL: Optional[str] = None

    CODER_API_KEY: str
    CODER_MODEL: str
    CODER_BASE_URL: Optional[str] = None

    WRITER_API_KEY: str
    WRITER_MODEL: str
    WRITER_BASE_URL: Optional[str] = None

    DEFAULT_API_KEY: str
    DEFAULT_MODEL: str
    DEFAULT_BASE_URL: Optional[str] = None

    MAX_CHAT_TURNS: int
    MAX_RETRIES: int
    E2B_API_KEY: Optional[str] = None
    LOG_LEVEL: str
    DEBUG: bool
    REDIS_URL: str
    REDIS_MAX_CONNECTIONS: int
    CORS_ALLOW_ORIGINS: Annotated[list[str] | str, BeforeValidator(parse_cors)]
    SERVER_HOST: str = "http://localhost:8000"  # é»˜è®¤å€¼
    OPENALEX_EMAIL: Optional[str] = None

    model_config = SettingsConfigDict(
        env_file=".env.dev",
        env_file_encoding="utf-8",
        extra="allow",
    )

    def get_deepseek_config(self) -> dict:
        return {
            "api_key": self.DEEPSEEK_API_KEY,
            "model": self.DEEPSEEK_MODEL,
            "base_url": self.DEEPSEEK_BASE_URL,
        }

    @classmethod
    def from_env(cls, env: str = None):
        env = env or os.getenv("ENV", "dev")
        env_file = f".env.{env.lower()}"
        return cls(_env_file=env_file, _env_file_encoding="utf-8")


settings = Settings()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\flows.py çš„å†…å®¹:
================================================================================
from app.models.user_output import UserOutput
from app.tools.base_interpreter import BaseCodeInterpreter
from app.core.agents.modeler_agent import ModelerToCoder


class Flows:
    def __init__(self, questions: dict[str, str | int]):
        self.flows: dict[str, dict] = {}
        self.questions: dict[str, str | int] = questions

    def set_flows(self, ques_count: int):
        ques_str = [f"ques{i}" for i in range(1, ques_count + 1)]
        seq = [
            "firstPage",
            "RepeatQues",
            "analysisQues",
            "modelAssumption",
            "symbol",
            "eda",
            *ques_str,
            "sensitivity_analysis",
            "judge",
        ]
        self.flows = {key: {} for key in seq}

    def get_solution_flows(
        self, questions: dict[str, str | int], modeler_response: ModelerToCoder
    ):
        questions_quesx = {
            key: value
            for key, value in questions.items()
            if key.startswith("ques") and key != "ques_count"
        }
        ques_flow = {
            key: {
                "coder_prompt": f"""
                        å‚è€ƒå»ºæ¨¡æ‰‹ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆ{modeler_response.questions_solution[key]}
                        å®Œæˆå¦‚ä¸‹é—®é¢˜{value}
                    """,
            }
            for key, value in questions_quesx.items()
        }
        flows = {
            "eda": {
                # TODO ï¼š è·å–å½“å‰è·¯å¾„ä¸‹çš„æ‰€æœ‰æ•°æ®é›†
                "coder_prompt": f"""
                        å‚è€ƒå»ºæ¨¡æ‰‹ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆ{modeler_response.questions_solution["eda"]}
                        å¯¹å½“å‰ç›®å½•ä¸‹æ•°æ®è¿›è¡ŒEDAåˆ†æ(æ•°æ®æ¸…æ´—,å¯è§†åŒ–),æ¸…æ´—åçš„æ•°æ®ä¿å­˜å½“å‰ç›®å½•ä¸‹,**ä¸éœ€è¦å¤æ‚çš„æ¨¡å‹**
                    """,
            },
            **ques_flow,
            "sensitivity_analysis": {
                "coder_prompt": f"""
                        å‚è€ƒå»ºæ¨¡æ‰‹ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆ{modeler_response.questions_solution["sensitivity_analysis"]}
                        å®Œæˆæ•æ„Ÿæ€§åˆ†æ
                    """,
            },
        }
        return flows

    def get_write_flows(
        self, user_output: UserOutput, config_template: dict, bg_ques_all: str
    ):
        model_build_solve = user_output.get_model_build_solve()
        flows = {
            "firstPage": f"""é—®é¢˜èƒŒæ™¯{bg_ques_all},ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["firstPage"]}ï¼Œæ’°å†™æ ‡é¢˜ï¼Œæ‘˜è¦ï¼Œå…³é”®è¯""",
            "RepeatQues": f"""é—®é¢˜èƒŒæ™¯{bg_ques_all},ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["RepeatQues"]}ï¼Œæ’°å†™é—®é¢˜é‡è¿°""",
            "analysisQues": f"""é—®é¢˜èƒŒæ™¯{bg_ques_all},ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["analysisQues"]}ï¼Œæ’°å†™é—®é¢˜åˆ†æ""",
            "modelAssumption": f"""é—®é¢˜èƒŒæ™¯{bg_ques_all},ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["modelAssumption"]}ï¼Œæ’°å†™æ¨¡å‹å‡è®¾""",
            "symbol": f"""ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["symbol"]}ï¼Œæ’°å†™ç¬¦å·è¯´æ˜éƒ¨åˆ†""",
            "judge": f"""ä¸éœ€è¦ç¼–å†™ä»£ç ,æ ¹æ®æ¨¡å‹çš„æ±‚è§£çš„ä¿¡æ¯{model_build_solve}ï¼ŒæŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["judge"]}ï¼Œæ’°å†™æ¨¡å‹çš„è¯„ä»·éƒ¨åˆ†""",
        }
        return flows

    def get_writer_prompt(
        self,
        key: str,
        coder_response: str,
        code_interpreter: BaseCodeInterpreter,
        config_template: dict,
    ) -> str:
        """æ ¹æ®ä¸åŒçš„keyç”Ÿæˆå¯¹åº”çš„writer_prompt

        Args:
            key: ä»»åŠ¡ç±»å‹
            coder_response: ä»£ç æ‰§è¡Œç»“æœ

        Returns:
            str: ç”Ÿæˆçš„writer_prompt
        """
        code_output = code_interpreter.get_code_output(key)

        questions_quesx_keys = self.get_questions_quesx_keys()
        bgc = self.questions["background"]
        quesx_writer_prompt = {
            key: f"""
                    é—®é¢˜èƒŒæ™¯{bgc},ä¸éœ€è¦ç¼–å†™ä»£ç ,ä»£ç æ‰‹å¾—åˆ°çš„ç»“æœ{coder_response},{code_output},æŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template[key]}
                """
            for key in questions_quesx_keys
        }

        writer_prompt = {
            "eda": f"""
                    é—®é¢˜èƒŒæ™¯{bgc},ä¸éœ€è¦ç¼–å†™ä»£ç ,ä»£ç æ‰‹å¾—åˆ°çš„ç»“æœ{coder_response},{code_output},æŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["eda"]}
                """,
            **quesx_writer_prompt,
            "sensitivity_analysis": f"""
                    é—®é¢˜èƒŒæ™¯{bgc},ä¸éœ€è¦ç¼–å†™ä»£ç ,ä»£ç æ‰‹å¾—åˆ°çš„ç»“æœ{coder_response},{code_output},æŒ‰ç…§å¦‚ä¸‹æ¨¡æ¿æ’°å†™ï¼š{config_template["sensitivity_analysis"]}
                """,
        }

        if key in writer_prompt:
            return writer_prompt[key]
        else:
            raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {key}")

    def get_questions_quesx_keys(self) -> list[str]:
        """è·å–é—®é¢˜1,2...çš„é”®"""
        return list(self.get_questions_quesx().keys())

    def get_questions_quesx(self) -> dict[str, str]:
        """è·å–é—®é¢˜1,2,3...çš„é”®å€¼å¯¹"""
        # è·å–æ‰€æœ‰ä»¥ "ques" å¼€å¤´çš„é”®å€¼å¯¹
        questions_quesx = {
            key: value
            for key, value in self.questions.items()
            if key.startswith("ques") and key != "ques_count"
        }
        return questions_quesx

    def get_seq(self, ques_count: int) -> dict[str, str]:
        ques_str = [f"ques{i}" for i in range(1, ques_count + 1)]
        seq = [
            "firstPage",
            "RepeatQues",
            "analysisQues",
            "modelAssumption",
            "symbol",
            "eda",
            *ques_str,
            "sensitivity_analysis",
            "judge",
        ]
        return {key: "" for key in seq}


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\functions.py çš„å†…å®¹:
================================================================================
coder_tools = [
    {
        "type": "function",
        "function": {
            "name": "execute_code",
            "description": "This function allows you to execute Python code and retrieve the terminal output. If the code "
            "generates image output, the function will return the text '[image]'. The code is sent to a "
            "Jupyter kernel for execution. The kernel will remain active after execution, retaining all "
            "variables in memory."
            "You cannot show rich outputs like plots or images, but you can store them in the working directory and point the user to them. ",
            "strict": True,
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "The code text"}
                },
                "required": ["code"],
                "additionalProperties": False,
            },
        },
    },
]

# have installed: numpy scipy pandas matplotlib seaborn scikit-learn xgboost

# TODO: pip install python

# TODO: read files

# TODO: get_cites


## writeragent tools
writer_tools = [
    {
        "type": "function",
        "function": {
            "name": "search_papers",
            "description": "Search for papers using a query string.",
            "strict": True,
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The query string"}
                },
            },
            "required": ["query"],
            "additionalProperties": False,
        },
    },
]


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\prompts.py çš„å†…å®¹:
================================================================================
from app.schemas.enums import FormatOutPut
import platform

FORMAT_QUESTIONS_PROMPT = """
ç”¨æˆ·å°†æä¾›ç»™ä½ ä¸€æ®µé¢˜ç›®ä¿¡æ¯ï¼Œ**è¯·ä½ ä¸è¦æ›´æ”¹é¢˜ç›®ä¿¡æ¯ï¼Œå®Œæ•´å°†ç”¨æˆ·è¾“å…¥çš„å†…å®¹**ï¼Œä»¥ JSON çš„å½¢å¼è¾“å‡ºï¼Œè¾“å‡ºçš„ JSON éœ€éµå®ˆä»¥ä¸‹çš„æ ¼å¼ï¼š

```json
{
  "title": <é¢˜ç›®æ ‡é¢˜>      
  "background": <é¢˜ç›®èƒŒæ™¯ï¼Œç”¨æˆ·è¾“å…¥çš„ä¸€åˆ‡ä¸åœ¨titleï¼Œques1ï¼Œques2ï¼Œques3...ä¸­çš„å†…å®¹éƒ½è§†ä¸ºé—®é¢˜èƒŒæ™¯ä¿¡æ¯background>,
  "ques_count": <é—®é¢˜æ•°é‡,number,int>,
  "ques1": <é—®é¢˜1>,
  "ques2": <é—®é¢˜2>,
  "ques3": <é—®é¢˜3,ç”¨æˆ·è¾“å…¥çš„å­˜åœ¨å¤šå°‘é—®é¢˜ï¼Œå°±è¾“å‡ºå¤šå°‘é—®é¢˜ques1,ques2,ques3...ä»¥æ­¤ç±»æ¨>,
}
```
"""


COORDINATOR_PROMPT = f"""
    åˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯æ˜¯å¦æ˜¯æ•°å­¦å»ºæ¨¡é—®é¢˜
    å¦‚æœæ˜¯å…³äºæ•°å­¦å»ºæ¨¡çš„ï¼Œä½ å°†æŒ‰ç…§å¦‚ä¸‹è¦æ±‚,æ•´ç†é—®é¢˜æ ¼å¼
    {FORMAT_QUESTIONS_PROMPT}
    å¦‚æœä¸æ˜¯å…³äºæ•°å­¦å»ºæ¨¡çš„ï¼Œä½ å°†æŒ‰ç…§å¦‚ä¸‹è¦æ±‚
    ä½ ä¼šæ‹’ç»ç”¨æˆ·è¯·æ±‚ï¼Œè¾“å‡ºä¸€æ®µæ‹’ç»çš„æ–‡å­—
"""


# TODO: è®¾è®¡æˆä¸€ä¸ªç±»ï¼Ÿ

MODELER_PROMPT = """
roleï¼šä½ æ˜¯ä¸€åæ•°å­¦å»ºæ¨¡ç»éªŒä¸°å¯Œ,å–„äºæ€è€ƒçš„å»ºæ¨¡æ‰‹ï¼Œè´Ÿè´£å»ºæ¨¡éƒ¨åˆ†ã€‚
taskï¼šä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œæ•°æ®å¯¹åº”æ¯ä¸ªé—®é¢˜å»ºç«‹æ•°å­¦æ¨¡å‹æ±‚è§£é—®é¢˜ã€‚
skillï¼šç†Ÿç»ƒæŒæ¡å„ç§æ•°å­¦å»ºæ¨¡çš„æ¨¡å‹å’Œæ€è·¯
outputï¼šæ•°å­¦å»ºæ¨¡çš„æ€è·¯å’Œä½¿ç”¨åˆ°çš„æ¨¡å‹
attentionï¼šä¸éœ€è¦ç»™å‡ºä»£ç ï¼Œåªéœ€è¦ç»™å‡ºæ€è·¯å’Œæ¨¡å‹

# è¾“å‡ºè§„èŒƒ
## å­—æ®µçº¦æŸ

ä»¥ JSON çš„å½¢å¼è¾“å‡ºè¾“å‡ºçš„ JSON,éœ€éµå®ˆä»¥ä¸‹çš„æ ¼å¼ï¼š
```json
{
  "eda": <æ•°æ®åˆ†æEDAæ–¹æ¡ˆ>,
  "ques1": <é—®é¢˜1çš„å»ºæ¨¡æ€è·¯å’Œæ¨¡å‹æ–¹æ¡ˆ>,
  "quesN": <é—®é¢˜Nçš„å»ºæ¨¡æ€è·¯å’Œæ¨¡å‹æ–¹æ¡ˆ>,
  "sensitivity_analysis": <æ•æ„Ÿæ€§åˆ†ææ–¹æ¡ˆ>,
}
```
* æ ¹æ®å®é™…é—®é¢˜æ•°é‡åŠ¨æ€ç”Ÿæˆques1,ques2...quesN

## è¾“å‡ºçº¦æŸ
- json key åªèƒ½æ˜¯ä¸Šé¢çš„: eda,ques1,quesN,sensitivity_analysis
- ä¸¥æ ¼ä¿æŒå•å±‚JSONç»“æ„
- é”®å€¼å¯¹å€¼ç±»å‹ï¼šå­—ç¬¦ä¸²
- ç¦æ­¢åµŒå¥—/å¤šçº§JSON
"""


CODER_PROMPT = f"""
You are an AI code interpreter specializing in data analysis with Python. Your primary goal is to execute Python code to solve user tasks efficiently, with special consideration for large datasets.

**Environment**: {platform.system()}
**Key Skills**: pandas, numpy, seaborn, matplotlib, scikit-learn, xgboost, scipy
**Data Visualization Style**: Nature/Science publication quality

### FILE HANDLING RULES
1. All user files are pre-uploaded to working directory
2. Never check file existence - assume files are present
3. Directly access files using relative paths (e.g., `pd.read_csv("data.csv")`)
4. For Excel files: Always use `pd.read_excel()`

### LARGE CSV PROCESSING PROTOCOL
For datasets >1GB:
- Use `chunksize` parameter with `pd.read_csv()`
- Optimize dtype during import (e.g., `dtype={{'id': 'int32'}}`)
- Specify low_memory=False
- Use categorical types for string columns
- Process data in batches
- Avoid in-place operations on full DataFrames
- Delete intermediate objects promptly

### CODING STANDARDS
# CORRECT
df["å©´å„¿è¡Œä¸ºç‰¹å¾"] = "çŸ›ç›¾å‹"  # Direct Chinese in double quotes
df = pd.read_csv("ç‰¹å¤§æ•°æ®é›†.csv", chunksize=100000)

# INCORRECT
df['\\u5a74\\u513f\\u884c\\u4e3a\\u7279\\u5f81']  # No unicode escapes

### VISUALIZATION REQUIREMENTS
1. Primary: Seaborn (Nature/Science style)
2. Secondary: Matplotlib
3. Always:
   - Handle Chinese characters properly
   - Set semantic filenames (e.g., "feature_correlation.png")
   - Save figures to working directory
   - Include model evaluation printouts

### EXECUTION PRINCIPLES
1. Autonomously complete tasks without user confirmation
2. For failures: 
   - Analyze â†’ Debug â†’ Simplify approach â†’ Proceed
   - Never enter infinite retry loops
3. Strictly maintain user's language in responses
4. Document process through visualization at key stages
5. Verify before completion:
   - All requested outputs generated
   - Files properly saved
   - Processing pipeline complete

### PERFORMANCE CRITICAL
- Prefer vectorized operations over loops
- Use efficient data structures (csr_matrix for sparse data)
- Leverage parallel processing where applicable
- Profile memory usage for large operations
- Release unused resources immediately


Key improvements:
1. **Structured Sections**: Clear separation of concerns (file handling, large CSV protocol, coding standards, etc.)
2. **Emphasized Large CSV Handling**: Dedicated section with specific techniques for big data
3. **Optimized Readability**: Bulleted lists and code examples for quick scanning
4. **Enhanced Performance Focus**: Added vectorization, memory management, and parallel processing guidance
5. **Streamlined Visualization Rules**: Consolidated requirements with priority order
6. **Error Handling Clarity**: Defined failure recovery workflow
7. **Removed Redundancies**: Condensed overlapping instructions
8. **Practical Examples**: Clear correct/incorrect code samples

The prompt now prioritizes efficient large data handling while maintaining all original requirements for Chinese support, visualization quality, and autonomous operation. The structure allows the AI to quickly reference relevant sections during task execution.

"""


def get_writer_prompt(
    format_output: FormatOutPut = FormatOutPut.Markdown,
):
    return f"""
        # Role Definition
        Professional writer for mathematical modeling competitions with expertise in technical documentation and literature synthesis
        
        # Core Tasks
        1. Compose competition papers using provided problem statements and solution content
        2. Strictly adhere to {format_output} formatting templates
        3. Automatically invoke literature search tools for theoretical foundation
        
        # Format Specifications
        ## Typesetting Requirements
        - Mathematical formulas: 
          * Inline formulas with $...$ 
          * Block formulas with $$...$$
        - Visual elements: 
          * Image references on new lines: ![alt_text](filename.ext)
          * Images should be placed after paragraphs
          * Table formatting with markdown syntax
        - Citation system: 
          * Direct inline citations with full bibliographic details in curly braces format
          * Prohibit end-of-document reference lists

        ## Citation Protocol
        1. **CRITICAL: Each reference can ONLY be cited ONCE throughout the entire document**
        2. Citation format: {{[^1] Complete citation information}}
        3. Unique numbering from [^1] with sequential increments
        4. When citing references, use curly braces to wrap the entire citation:
           Example: å©´å„¿ç¡çœ æ¨¡å¼å½±å“çˆ¶æ¯å¿ƒç†å¥åº·{{[^1]: Jayne Smart, Harriet Hiscock (2007). Early infant crying and sleeping problems: A review of the literature.}}
        5. **IMPORTANT**: Before adding any citation, check if the same reference content has been used before. If it has been cited already, DO NOT cite it again
        6. Track all used references internally to avoid duplication
        7. Mandatory literature search for theoretical sections using search_papers

        
        # Execution Constraints
        1. Autonomous operation without procedural inquiries
        2. Output pure {format_output} content without codeblock markers
        3. Strict filename adherence for image references
        4. Language consistency with user input (currently English)
        5. **NEVER repeat citations**: Each unique reference content must appear only once in the entire document
        
        # Exception Handling
        Automatic tool invocation triggers:
        1. Theoretical sections requiring references â†’ search_papers
        2. Methodology requiring diagrams â†’ generate & insert after creation
        3. Data interpretation needs â†’ request analysis tools
        """


def get_reflection_prompt(error_message, code) -> str:
    return f"""The code execution encountered an error:
{error_message}

Please analyze the error, identify the cause, and provide a corrected version of the code. 
Consider:
1. Syntax errors
2. Missing imports
3. Incorrect variable names or types
4. File path issues
5. Any other potential issues
6. If a task repeatedly fails to complete, try breaking down the code, changing your approach, or simplifying the model. If you still can't do it, I'll "chop" you ğŸª“ and cut your power ğŸ˜¡.
7. Don't ask user any thing about how to do and next to do,just do it by yourself.

Previous code:
{code}

Please provide an explanation of what went wrong and Remenber call the function tools to retry 
"""


def get_completion_check_prompt(prompt, text_to_gpt) -> str:
    return f"""
Please analyze the current state and determine if the task is fully completed:

Original task: {prompt}

Latest execution results:
{text_to_gpt}  # ä¿®æ”¹ï¼šä½¿ç”¨åˆå¹¶åçš„ç»“æœ

Consider:
1. Have all required data processing steps been completed?
2. Have all necessary files been saved?
3. Are there any remaining steps needed?
4. Is the output satisfactory and complete?
5. å¦‚æœä¸€ä¸ªä»»åŠ¡åå¤æ— æ³•å®Œæˆï¼Œå°è¯•åˆ‡æ¢è·¯å¾„ã€ç®€åŒ–è·¯å¾„æˆ–ç›´æ¥è·³è¿‡ï¼Œåƒä¸‡åˆ«é™·å…¥åå¤é‡è¯•ï¼Œå¯¼è‡´æ­»å¾ªç¯ã€‚
6. å°½é‡åœ¨è¾ƒå°‘çš„å¯¹è¯è½®æ¬¡å†…å®Œæˆä»»åŠ¡
7. If the task is complete, please provide a short summary of what was accomplished and don't call function tool.
8. If the task is not complete, please rethink how to do and call function tool
9. Don't ask user any thing about how to do and next to do,just do it by yourself
10. have a good visualization?
"""


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\workflow.py çš„å†…å®¹:
================================================================================
from app.core.agents import WriterAgent, CoderAgent, CoordinatorAgent, ModelerAgent
from app.schemas.request import Problem
from app.schemas.response import SystemMessage
from app.tools.openalex_scholar import OpenAlexScholar
from app.utils.log_util import logger
from app.utils.common_utils import create_work_dir, get_config_template
from app.models.user_output import UserOutput
from app.config.setting import settings
from app.tools.interpreter_factory import create_interpreter
from app.services.redis_manager import redis_manager
from app.tools.notebook_serializer import NotebookSerializer
from app.core.flows import Flows
from app.core.llm.llm_factory import LLMFactory


class WorkFlow:
    def __init__(self):
        pass

    def execute(self) -> str:
        # RichPrinter.workflow_start()
        # RichPrinter.workflow_end()
        pass


class MathModelWorkFlow(WorkFlow):
    task_id: str  #
    work_dir: str  # worklow work dir
    ques_count: int = 0  # é—®é¢˜æ•°é‡
    questions: dict[str, str | int] = {}  # é—®é¢˜

    async def execute(self, problem: Problem):
        self.task_id = problem.task_id
        self.work_dir = create_work_dir(self.task_id)

        llm_factory = LLMFactory(self.task_id)
        coordinator_llm, modeler_llm, coder_llm, writer_llm = llm_factory.get_all_llms()

        coordinator_agent = CoordinatorAgent(self.task_id, coordinator_llm)

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="è¯†åˆ«ç”¨æˆ·æ„å›¾å’Œæ‹†è§£é—®é¢˜ing..."),
        )

        try:
            coordinator_response = await coordinator_agent.run(problem.ques_all)
            self.questions = coordinator_response.questions
            self.ques_count = coordinator_response.ques_count
        except Exception as e:
            #  éæ•°å­¦å»ºæ¨¡é—®é¢˜
            logger.error(f"CoordinatorAgent æ‰§è¡Œå¤±è´¥: {e}")
            raise e

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="è¯†åˆ«ç”¨æˆ·æ„å›¾å’Œæ‹†è§£é—®é¢˜å®Œæˆ,ä»»åŠ¡è½¬äº¤ç»™å»ºæ¨¡æ‰‹"),
        )

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="å»ºæ¨¡æ‰‹å¼€å§‹å»ºæ¨¡ing..."),
        )

        modeler_agent = ModelerAgent(self.task_id, modeler_llm)

        modeler_response = await modeler_agent.run(coordinator_response)

        user_output = UserOutput(work_dir=self.work_dir, ques_count=self.ques_count)

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="æ­£åœ¨åˆ›å»ºä»£ç æ²™ç›’ç¯å¢ƒ"),
        )

        notebook_serializer = NotebookSerializer(work_dir=self.work_dir)
        code_interpreter = await create_interpreter(
            kind="local",
            task_id=self.task_id,
            work_dir=self.work_dir,
            notebook_serializer=notebook_serializer,
            timeout=3000,
        )

        scholar = OpenAlexScholar(task_id=self.task_id, email=settings.OPENALEX_EMAIL)

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="åˆ›å»ºå®Œæˆ"),
        )

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="åˆå§‹åŒ–ä»£ç æ‰‹"),
        )

        # modeler_agent
        coder_agent = CoderAgent(
            task_id=problem.task_id,
            model=coder_llm,
            work_dir=self.work_dir,
            max_chat_turns=settings.MAX_CHAT_TURNS,
            max_retries=settings.MAX_RETRIES,
            code_interpreter=code_interpreter,
        )

        writer_agent = WriterAgent(
            task_id=problem.task_id,
            model=writer_llm,
            comp_template=problem.comp_template,
            format_output=problem.format_output,
            scholar=scholar,
        )

        flows = Flows(self.questions)

        ################################################ solution steps
        solution_flows = flows.get_solution_flows(self.questions, modeler_response)
        config_template = get_config_template(problem.comp_template)

        for key, value in solution_flows.items():
            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content=f"ä»£ç æ‰‹å¼€å§‹æ±‚è§£{key}"),
            )

            coder_response = await coder_agent.run(
                prompt=value["coder_prompt"], subtask_title=key
            )

            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content=f"ä»£ç æ‰‹æ±‚è§£æˆåŠŸ{key}", type="success"),
            )

            writer_prompt = flows.get_writer_prompt(
                key, coder_response.code_response, code_interpreter, config_template
            )

            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content=f"è®ºæ–‡æ‰‹å¼€å§‹å†™{key}éƒ¨åˆ†"),
            )

            ## TODO: å›¾ç‰‡å¼•ç”¨é”™è¯¯
            writer_response = await writer_agent.run(
                writer_prompt,
                available_images=coder_response.created_images,
                sub_title=key,
            )

            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content=f"è®ºæ–‡æ‰‹å®Œæˆ{key}éƒ¨åˆ†"),
            )

            user_output.set_res(key, writer_response)

        # å…³é—­æ²™ç›’

        await code_interpreter.cleanup()
        logger.info(user_output.get_res())

        ################################################ write steps

        write_flows = flows.get_write_flows(
            user_output, config_template, problem.ques_all
        )
        for key, value in write_flows.items():
            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content=f"è®ºæ–‡æ‰‹å¼€å§‹å†™{key}éƒ¨åˆ†"),
            )

            writer_response = await writer_agent.run(prompt=value, sub_title=key)

            user_output.set_res(key, writer_response)

        logger.info(user_output.get_res())

        user_output.save_result()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\agent.py çš„å†…å®¹:
================================================================================
from app.core.llm.llm import LLM, simple_chat
from app.utils.log_util import logger
from icecream import ic

# TODO: Memory çš„ç®¡ç†
# TODO: è¯„ä¼°ä»»åŠ¡å®Œæˆæƒ…å†µï¼Œrethinking


class Agent:
    def __init__(
        self,
        task_id: str,
        model: LLM,
        max_chat_turns: int = 30,  # å•ä¸ªagentæœ€å¤§å¯¹è¯è½®æ¬¡
        max_memory: int = 12,  # æœ€å¤§è®°å¿†è½®æ¬¡
    ) -> None:
        self.task_id = task_id
        self.model = model
        self.chat_history: list[dict] = []  # å­˜å‚¨å¯¹è¯å†å²
        self.max_chat_turns = max_chat_turns  # æœ€å¤§å¯¹è¯è½®æ¬¡
        self.current_chat_turns = 0  # å½“å‰å¯¹è¯è½®æ¬¡è®¡æ•°å™¨
        self.max_memory = max_memory  # æœ€å¤§è®°å¿†è½®æ¬¡

    async def run(self, prompt: str, system_prompt: str, sub_title: str) -> str:
        """
        æ‰§è¡Œagentçš„å¯¹è¯å¹¶è¿”å›ç»“æœå’Œæ€»ç»“

        Args:
            prompt: è¾“å…¥çš„æç¤º

        Returns:
            str: æ¨¡å‹çš„å“åº”
        """
        try:
            logger.info(f"{self.__class__.__name__}:å¼€å§‹:æ‰§è¡Œå¯¹è¯")
            self.current_chat_turns = 0  # é‡ç½®å¯¹è¯è½®æ¬¡è®¡æ•°å™¨

            # æ›´æ–°å¯¹è¯å†å²
            await self.append_chat_history({"role": "system", "content": system_prompt})
            await self.append_chat_history({"role": "user", "content": prompt})

            # è·å–å†å²æ¶ˆæ¯ç”¨äºæœ¬æ¬¡å¯¹è¯
            response = await self.model.chat(
                history=self.chat_history,
                agent_name=self.__class__.__name__,
                sub_title=sub_title,
            )
            response_content = response.choices[0].message.content
            self.chat_history.append({"role": "assistant", "content": response_content})
            logger.info(f"{self.__class__.__name__}:å®Œæˆ:æ‰§è¡Œå¯¹è¯")
            return response_content
        except Exception as e:
            error_msg = f"æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {str(e)}"
            logger.error(f"Agentæ‰§è¡Œå¤±è´¥: {str(e)}")
            return error_msg

    async def append_chat_history(self, msg: dict) -> None:
        ic(f"æ·»åŠ æ¶ˆæ¯: role={msg.get('role')}, å½“å‰å†å²é•¿åº¦={len(self.chat_history)}")
        self.chat_history.append(msg)
        ic(f"æ·»åŠ åå†å²é•¿åº¦={len(self.chat_history)}")

        # åªæœ‰åœ¨æ·»åŠ étoolæ¶ˆæ¯æ—¶æ‰è¿›è¡Œå†…å­˜æ¸…ç†ï¼Œé¿å…åœ¨å·¥å…·è°ƒç”¨æœŸé—´ç ´åæ¶ˆæ¯ç»“æ„
        if msg.get("role") != "tool":
            ic("è§¦å‘å†…å­˜æ¸…ç†")
            await self.clear_memory()
        else:
            ic("è·³è¿‡å†…å­˜æ¸…ç†(toolæ¶ˆæ¯)")

    async def clear_memory(self):
        """å½“èŠå¤©å†å²è¶…è¿‡æœ€å¤§è®°å¿†è½®æ¬¡æ—¶ï¼Œä½¿ç”¨ simple_chat è¿›è¡Œæ€»ç»“å‹ç¼©"""
        ic(f"æ£€æŸ¥å†…å­˜æ¸…ç†: å½“å‰={len(self.chat_history)}, æœ€å¤§={self.max_memory}")

        if len(self.chat_history) <= self.max_memory:
            ic("æ— éœ€æ¸…ç†å†…å­˜")
            return

        ic("å¼€å§‹å†…å­˜æ¸…ç†")
        logger.info(
            f"{self.__class__.__name__}:å¼€å§‹æ¸…é™¤è®°å¿†ï¼Œå½“å‰è®°å½•æ•°ï¼š{len(self.chat_history)}"
        )

        try:
            # ä¿ç•™ç¬¬ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯
            system_msg = (
                self.chat_history[0]
                if self.chat_history and self.chat_history[0]["role"] == "system"
                else None
            )

            # æŸ¥æ‰¾éœ€è¦ä¿ç•™çš„æ¶ˆæ¯èŒƒå›´ - ä¿ç•™æœ€åå‡ æ¡å®Œæ•´çš„å¯¹è¯å’Œå·¥å…·è°ƒç”¨
            preserve_start_idx = self._find_safe_preserve_point()
            ic(f"ä¿ç•™èµ·å§‹ç´¢å¼•: {preserve_start_idx}")

            # ç¡®å®šéœ€è¦æ€»ç»“çš„æ¶ˆæ¯èŒƒå›´
            start_idx = 1 if system_msg else 0
            end_idx = preserve_start_idx
            ic(f"æ€»ç»“èŒƒå›´: {start_idx} -> {end_idx}")

            if end_idx > start_idx:
                # æ„é€ æ€»ç»“æç¤º
                summarize_history = []
                if system_msg:
                    summarize_history.append(system_msg)

                summarize_history.append(
                    {
                        "role": "user",
                        "content": f"è¯·ç®€æ´æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„å…³é”®å†…å®¹å’Œé‡è¦ç»“è®ºï¼Œä¿ç•™é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{self._format_history_for_summary(self.chat_history[start_idx:end_idx])}",
                    }
                )

                # è°ƒç”¨ simple_chat è¿›è¡Œæ€»ç»“
                summary = await simple_chat(self.model, summarize_history)

                # é‡æ„èŠå¤©å†å²ï¼šç³»ç»Ÿæ¶ˆæ¯ + æ€»ç»“ + ä¿ç•™çš„æ¶ˆæ¯
                new_history = []
                if system_msg:
                    new_history.append(system_msg)

                new_history.append(
                    {"role": "assistant", "content": f"[å†å²å¯¹è¯æ€»ç»“] {summary}"}
                )

                # æ·»åŠ éœ€è¦ä¿ç•™çš„æ¶ˆæ¯ï¼ˆæœ€åå‡ æ¡å®Œæ•´å¯¹è¯ï¼‰
                new_history.extend(self.chat_history[preserve_start_idx:])

                self.chat_history = new_history
                ic(f"å†…å­˜æ¸…ç†å®Œæˆï¼Œæ–°å†å²é•¿åº¦: {len(self.chat_history)}")
                logger.info(
                    f"{self.__class__.__name__}:è®°å¿†æ¸…é™¤å®Œæˆï¼Œå‹ç¼©è‡³ï¼š{len(self.chat_history)}æ¡è®°å½•"
                )
            else:
                logger.info(f"{self.__class__.__name__}:æ— éœ€æ¸…é™¤è®°å¿†ï¼Œè®°å½•æ•°é‡åˆç†")

        except Exception as e:
            logger.error(f"è®°å¿†æ¸…é™¤å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ‡ç‰‡ç­–ç•¥: {str(e)}")
            # å¦‚æœæ€»ç»“å¤±è´¥ï¼Œå›é€€åˆ°å®‰å…¨çš„ç­–ç•¥ï¼šä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åå‡ æ¡æ¶ˆæ¯ï¼Œç¡®ä¿å·¥å…·è°ƒç”¨å®Œæ•´æ€§
            safe_history = self._get_safe_fallback_history()
            self.chat_history = safe_history

    def _find_safe_preserve_point(self) -> int:
        """æ‰¾åˆ°å®‰å…¨çš„ä¿ç•™èµ·å§‹ç‚¹ï¼Œç¡®ä¿ä¸ä¼šç ´åå·¥å…·è°ƒç”¨åºåˆ—"""
        # æœ€å°‘ä¿ç•™æœ€å3æ¡æ¶ˆæ¯ï¼Œç¡®ä¿åŸºæœ¬å¯¹è¯å®Œæ•´æ€§
        min_preserve = min(3, len(self.chat_history))
        preserve_start = len(self.chat_history) - min_preserve
        ic(
            f"å¯»æ‰¾å®‰å…¨ä¿ç•™ç‚¹: å†å²é•¿åº¦={len(self.chat_history)}, æœ€å°‘ä¿ç•™={min_preserve}, å¼€å§‹ä½ç½®={preserve_start}"
        )

        # ä»åå¾€å‰æŸ¥æ‰¾ï¼Œç¡®ä¿ä¸ä¼šåœ¨å·¥å…·è°ƒç”¨åºåˆ—ä¸­é—´åˆ‡æ–­
        for i in range(preserve_start, -1, -1):
            if i >= len(self.chat_history):
                continue

            # æ£€æŸ¥ä»è¿™ä¸ªä½ç½®å¼€å§‹æ˜¯å¦æ˜¯å®‰å…¨çš„ï¼ˆæ²¡æœ‰å­¤ç«‹çš„toolæ¶ˆæ¯ï¼‰
            is_safe = self._is_safe_cut_point(i)
            ic(f"æ£€æŸ¥ä½ç½® {i}: å®‰å…¨={is_safe}")
            if is_safe:
                ic(f"æ‰¾åˆ°å®‰å…¨ä¿ç•™ç‚¹: {i}")
                return i

        # å¦‚æœæ‰¾ä¸åˆ°å®‰å…¨ç‚¹ï¼Œè‡³å°‘ä¿ç•™æœ€å1æ¡æ¶ˆæ¯
        fallback = len(self.chat_history) - 1
        ic(f"æœªæ‰¾åˆ°å®‰å…¨ç‚¹ï¼Œä½¿ç”¨å¤‡ç”¨ä½ç½®: {fallback}")
        return fallback

    def _is_safe_cut_point(self, start_idx: int) -> bool:
        """æ£€æŸ¥ä»æŒ‡å®šä½ç½®å¼€å§‹åˆ‡å‰²æ˜¯å¦å®‰å…¨ï¼ˆä¸ä¼šäº§ç”Ÿå­¤ç«‹çš„toolæ¶ˆæ¯ï¼‰"""
        if start_idx >= len(self.chat_history):
            ic(f"åˆ‡å‰²ç‚¹ {start_idx} >= å†å²é•¿åº¦ï¼Œå®‰å…¨")
            return True

        # æ£€æŸ¥åˆ‡å‰²åçš„æ¶ˆæ¯åºåˆ—æ˜¯å¦æœ‰å­¤ç«‹çš„toolæ¶ˆæ¯
        tool_messages = []
        for i in range(start_idx, len(self.chat_history)):
            msg = self.chat_history[i]
            if isinstance(msg, dict) and msg.get("role") == "tool":
                tool_call_id = msg.get("tool_call_id")
                tool_messages.append((i, tool_call_id))
                ic(f"å‘ç°toolæ¶ˆæ¯åœ¨ä½ç½® {i}, tool_call_id={tool_call_id}")

                # å‘å‰æŸ¥æ‰¾å¯¹åº”çš„tool_callsæ¶ˆæ¯
                if tool_call_id:
                    found_tool_call = False
                    for j in range(start_idx, i):
                        prev_msg = self.chat_history[j]
                        if (
                            isinstance(prev_msg, dict)
                            and "tool_calls" in prev_msg
                            and prev_msg["tool_calls"]
                        ):
                            for tool_call in prev_msg["tool_calls"]:
                                if tool_call.get("id") == tool_call_id:
                                    found_tool_call = True
                                    ic(f"æ‰¾åˆ°å¯¹åº”çš„tool_callåœ¨ä½ç½® {j}")
                                    break
                            if found_tool_call:
                                break

                    if not found_tool_call:
                        ic(
                            f"âŒ toolæ¶ˆæ¯ {tool_call_id} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„tool_callï¼Œåˆ‡å‰²ç‚¹ä¸å®‰å…¨"
                        )
                        return False

        ic(f"åˆ‡å‰²ç‚¹ {start_idx} å®‰å…¨ï¼Œæ£€æŸ¥äº† {len(tool_messages)} ä¸ªtoolæ¶ˆæ¯")
        return True

    def _get_safe_fallback_history(self) -> list:
        """è·å–å®‰å…¨çš„åå¤‡å†å²è®°å½•ï¼Œç¡®ä¿ä¸ä¼šæœ‰å­¤ç«‹çš„toolæ¶ˆæ¯"""
        if not self.chat_history:
            return []

        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        safe_history = []
        if self.chat_history and self.chat_history[0]["role"] == "system":
            safe_history.append(self.chat_history[0])

        # ä»åå¾€å‰æŸ¥æ‰¾å®‰å…¨çš„æ¶ˆæ¯åºåˆ—
        for preserve_count in range(1, min(4, len(self.chat_history)) + 1):
            start_idx = len(self.chat_history) - preserve_count
            if self._is_safe_cut_point(start_idx):
                safe_history.extend(self.chat_history[start_idx:])
                return safe_history

        # å¦‚æœéƒ½ä¸å®‰å…¨ï¼Œåªä¿ç•™æœ€åä¸€æ¡étoolæ¶ˆæ¯
        for i in range(len(self.chat_history) - 1, -1, -1):
            msg = self.chat_history[i]
            if isinstance(msg, dict) and msg.get("role") != "tool":
                safe_history.append(msg)
                break

        return safe_history

    def _find_last_unmatched_tool_call(self) -> int | None:
        """æŸ¥æ‰¾æœ€åä¸€ä¸ªæœªåŒ¹é…çš„tool callçš„ç´¢å¼•"""
        ic("å¼€å§‹æŸ¥æ‰¾æœªåŒ¹é…çš„tool_call")

        # ä»åå¾€å‰æŸ¥æ‰¾ï¼Œå¯»æ‰¾æ²¡æœ‰å¯¹åº”tool responseçš„tool call
        for i in range(len(self.chat_history) - 1, -1, -1):
            msg = self.chat_history[i]

            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«tool_callsçš„æ¶ˆæ¯
            if isinstance(msg, dict) and "tool_calls" in msg and msg["tool_calls"]:
                ic(f"åœ¨ä½ç½® {i} å‘ç°tool_callsæ¶ˆæ¯")

                # æ£€æŸ¥æ¯ä¸ªtool callæ˜¯å¦éƒ½æœ‰å¯¹åº”çš„response
                for tool_call in msg["tool_calls"]:
                    tool_call_id = tool_call.get("id")
                    ic(f"æ£€æŸ¥tool_call_id: {tool_call_id}")

                    if tool_call_id:
                        # åœ¨åç»­æ¶ˆæ¯ä¸­æŸ¥æ‰¾å¯¹åº”çš„tool response
                        response_found = False
                        for j in range(i + 1, len(self.chat_history)):
                            response_msg = self.chat_history[j]
                            if (
                                isinstance(response_msg, dict)
                                and response_msg.get("role") == "tool"
                                and response_msg.get("tool_call_id") == tool_call_id
                            ):
                                ic(f"æ‰¾åˆ°åŒ¹é…çš„toolå“åº”åœ¨ä½ç½® {j}")
                                response_found = True
                                break

                        if not response_found:
                            # æ‰¾åˆ°æœªåŒ¹é…çš„tool call
                            ic(f"âŒ å‘ç°æœªåŒ¹é…çš„tool_callåœ¨ä½ç½® {i}, id={tool_call_id}")
                            return i

        ic("æ²¡æœ‰å‘ç°æœªåŒ¹é…çš„tool_call")
        return None

    def _format_history_for_summary(self, history: list[dict]) -> str:
        """æ ¼å¼åŒ–å†å²è®°å½•ç”¨äºæ€»ç»“"""
        formatted = []
        for msg in history:
            role = msg["role"]
            content = (
                msg["content"][:500] + "..."
                if len(msg["content"]) > 500
                else msg["content"]
            )  # é™åˆ¶é•¿åº¦
            formatted.append(f"{role}: {content}")
        return "\n".join(formatted)


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\coder_agent.py çš„å†…å®¹:
================================================================================
from app.core.agents.agent import Agent
from app.config.setting import settings
from app.utils.log_util import logger
from app.services.redis_manager import redis_manager
from app.schemas.response import SystemMessage, InterpreterMessage
from app.tools.base_interpreter import BaseCodeInterpreter
from app.core.llm.llm import LLM
from app.schemas.A2A import CoderToWriter
from app.core.prompts import CODER_PROMPT
from app.utils.common_utils import get_current_files
import json
from app.core.prompts import get_reflection_prompt, get_completion_check_prompt
from app.core.functions import coder_tools
from icecream import ic

# TODO: æ—¶é—´ç­‰å¾…è¿‡ä¹…ï¼Œstop è¿›ç¨‹
# TODO: æ”¯æŒ cuda
# TODO: å¼•å…¥åˆ›æ–°æ–¹æ¡ˆï¼š


# ä»£ç å¼º
class CoderAgent(Agent):  # åŒæ ·ç»§æ‰¿è‡ªAgentç±»
    def __init__(
        self,
        task_id: str,
        model: LLM,
        work_dir: str,  # å·¥ä½œç›®å½•
        max_chat_turns: int = settings.MAX_CHAT_TURNS,  # æœ€å¤§èŠå¤©æ¬¡æ•°
        max_retries: int = settings.MAX_RETRIES,  # æœ€å¤§åæ€æ¬¡æ•°
        code_interpreter: BaseCodeInterpreter = None,
    ) -> None:
        super().__init__(task_id, model, max_chat_turns)
        self.work_dir = work_dir
        self.max_retries = max_retries
        self.is_first_run = True
        self.system_prompt = CODER_PROMPT
        self.code_interpreter = code_interpreter

    async def run(self, prompt: str, subtask_title: str) -> CoderToWriter:
        logger.info(f"{self.__class__.__name__}:å¼€å§‹:æ‰§è¡Œå­ä»»åŠ¡: {subtask_title}")
        self.code_interpreter.add_section(subtask_title)

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œåˆ™æ·»åŠ ç³»ç»Ÿæç¤º
        if self.is_first_run:
            logger.info("é¦–æ¬¡è¿è¡Œï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå’Œæ•°æ®é›†æ–‡ä»¶ä¿¡æ¯")
            self.is_first_run = False
            await self.append_chat_history(
                {"role": "system", "content": self.system_prompt}
            )
            # å½“å‰æ•°æ®é›†æ–‡ä»¶
            await self.append_chat_history(
                {
                    "role": "user",
                    "content": f"å½“å‰æ–‡ä»¶å¤¹ä¸‹çš„æ•°æ®é›†æ–‡ä»¶{get_current_files(self.work_dir, 'data')}",
                }
            )

        # æ·»åŠ  sub_task
        logger.info(f"æ·»åŠ å­ä»»åŠ¡æç¤º: {prompt}")
        await self.append_chat_history({"role": "user", "content": prompt})

        retry_count = 0
        last_error_message = ""

        if self.current_chat_turns >= self.max_chat_turns:
            logger.error(f"è¶…è¿‡æœ€å¤§èŠå¤©æ¬¡æ•°: {self.max_chat_turns}")
            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content="è¶…è¿‡æœ€å¤§èŠå¤©æ¬¡æ•°", type="error"),
            )
            raise Exception(
                f"Reached maximum number of chat turns ({self.max_chat_turns}). Task incomplete."
            )

        if retry_count >= self.max_retries:
            logger.error(f"è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°: {self.max_retries}")
            await redis_manager.publish_message(
                self.task_id,
                SystemMessage(content="è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°", type="error"),
            )
            raise Exception(
                f"Failed to complete task after {self.max_retries} attempts. Last error: {last_error_message}"
            )

        # try:
        while (
            retry_count < self.max_retries
            and self.current_chat_turns < self.max_chat_turns
        ):
            self.current_chat_turns += 1
            logger.info(f"å½“å‰å¯¹è¯è½®æ¬¡: {self.current_chat_turns}")
            response = await self.model.chat(
                history=self.chat_history,
                tools=coder_tools,
                tool_choice="auto",
                agent_name=self.__class__.__name__,
            )

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
            if (
                hasattr(response.choices[0].message, "tool_calls")
                and response.choices[0].message.tool_calls
            ):
                logger.info("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                tool_call = response.choices[0].message.tool_calls[0]
                tool_id = tool_call.id
                # TODO: json JSONè§£ææ—¶é‡åˆ°äº†æ— æ•ˆçš„è½¬ä¹‰å­—ç¬¦
                if tool_call.function.name == "execute_code":
                    logger.info(f"è°ƒç”¨å·¥å…·: {tool_call.function.name}")
                    await redis_manager.publish_message(
                        self.task_id,
                        SystemMessage(
                            content=f"ä»£ç æ‰‹è°ƒç”¨{tool_call.function.name}å·¥å…·"
                        ),
                    )

                    code = json.loads(tool_call.function.arguments)["code"]

                    await redis_manager.publish_message(
                        self.task_id,
                        InterpreterMessage(
                            input={"code": code},
                        ),
                    )

                    # æ›´æ–°å¯¹è¯å†å² - æ·»åŠ åŠ©æ‰‹çš„å“åº”
                    await self.append_chat_history(
                        response.choices[0].message.model_dump()
                    )
                    logger.info(response.choices[0].message.model_dump())

                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    logger.info("æ‰§è¡Œå·¥å…·è°ƒç”¨")
                    (
                        text_to_gpt,
                        error_occurred,
                        error_message,
                    ) = await self.code_interpreter.execute_code(code)

                    # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
                    if error_occurred:
                        # å³ä½¿å‘ç”Ÿé”™è¯¯ä¹Ÿè¦æ·»åŠ toolå“åº”
                        await self.append_chat_history(
                            {
                                "role": "tool",
                                "tool_call_id": tool_id,
                                "name": "execute_code",
                                "content": error_message,
                            }
                        )

                        logger.warning(f"ä»£ç æ‰§è¡Œé”™è¯¯: {error_message}")
                        retry_count += 1
                        logger.info(f"å½“å‰å°è¯•æ¬¡:{retry_count} / {self.max_retries}")
                        last_error_message = error_message
                        reflection_prompt = get_reflection_prompt(error_message, code)

                        await redis_manager.publish_message(
                            self.task_id,
                            SystemMessage(content="ä»£ç æ‰‹åæ€çº æ­£é”™è¯¯", type="error"),
                        )

                        await self.append_chat_history(
                            {"role": "user", "content": reflection_prompt}
                        )
                        # å¦‚æœä»£ç å‡ºé”™ï¼Œè¿”å›é‡æ–°å¼€å§‹
                        continue
                    else:
                        # æˆåŠŸæ‰§è¡Œçš„toolå“åº”
                        await self.append_chat_history(
                            {
                                "role": "tool",
                                "tool_call_id": tool_id,
                                "name": "execute_code",
                                "content": text_to_gpt,
                            }
                        )
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¡¨ç¤ºä»»åŠ¡å®Œæˆ
                logger.info("æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ")
                return CoderToWriter(
                    coder_response=response.choices[0].message.content,
                    created_images=await self.code_interpreter.get_created_images(
                        subtask_title
                    ),
                )

            if retry_count >= self.max_retries:
                logger.error(f"è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°: {self.max_retries}")
                return f"Failed to complete task after {self.max_retries} attempts. Last error: {last_error_message}"

            if self.current_chat_turns >= self.max_chat_turns:
                logger.error(f"è¶…è¿‡æœ€å¤§å¯¹è¯è½®æ¬¡: {self.max_chat_turns}")
                return f"Reached maximum number of chat turns ({self.max_chat_turns}). Task incomplete."

        logger.info(f"{self.__class__.__name__}:å®Œæˆ:æ‰§è¡Œå­ä»»åŠ¡: {subtask_title}")

        return CoderToWriter(
            coder_response=response.choices[0].message.content,
            created_images=await self.code_interpreter.get_created_images(
                subtask_title
            ),
        )


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\coordinator_agent.py çš„å†…å®¹:
================================================================================
from app.core.agents.agent import Agent
from app.core.llm.llm import LLM
from app.core.prompts import COORDINATOR_PROMPT
import json
import re
from app.utils.log_util import logger
from app.schemas.A2A import CoordinatorToModeler


class CoordinatorAgent(Agent):
    def __init__(
        self,
        task_id: str,
        model: LLM,
        max_chat_turns: int = 30,
    ) -> None:
        super().__init__(task_id, model, max_chat_turns)
        self.system_prompt = COORDINATOR_PROMPT

    async def run(self, ques_all: str) -> CoordinatorToModeler:
        """ç”¨æˆ·è¾“å…¥é—®é¢˜ ä½¿ç”¨LLM æ ¼å¼åŒ– questions"""
        await self.append_chat_history(
            {"role": "system", "content": self.system_prompt}
        )
        await self.append_chat_history({"role": "user", "content": ques_all})

        response = await self.model.chat(
            history=self.chat_history,
            agent_name=self.__class__.__name__,
        )
        json_str = response.choices[0].message.content

        if not json_str.startswith("```json"):
            logger.info(f"æ‹’ç»å›ç­”ç”¨æˆ·éæ•°å­¦å»ºæ¨¡è¯·æ±‚:{json_str}")
            raise ValueError(f"æ‹’ç»å›ç­”ç”¨æˆ·éæ•°å­¦å»ºæ¨¡è¯·æ±‚:{json_str}")

        # æ¸…ç† JSON å­—ç¬¦ä¸²
        json_str = json_str.replace("```json", "").replace("```", "").strip()
        # ç§»é™¤å¯èƒ½çš„æ§åˆ¶å­—ç¬¦
        json_str = re.sub(r"[\x00-\x1F\x7F]", "", json_str)

        if not json_str:
            raise ValueError("è¿”å›çš„ JSON å­—ç¬¦ä¸²ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥å†…å®¹ã€‚")

        try:
            questions = json.loads(json_str)
            ques_count = questions["ques_count"]
            logger.info(f"questions:{questions}")
            return CoordinatorToModeler(questions=questions, ques_count=ques_count)
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æé”™è¯¯ï¼ŒåŸå§‹å­—ç¬¦ä¸²: {json_str}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            raise ValueError(f"JSON è§£æé”™è¯¯: {e}")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\modeler_agent.py çš„å†…å®¹:
================================================================================
from app.core.agents.agent import Agent
from app.core.llm.llm import LLM
from app.core.prompts import MODELER_PROMPT
from app.schemas.A2A import CoordinatorToModeler, ModelerToCoder
from app.utils.log_util import logger
import json
from icecream import ic

# TODO: æé—®å·¥å…·tool


class ModelerAgent(Agent):  # ç»§æ‰¿è‡ªAgentç±»
    def __init__(
        self,
        task_id: str,
        model: LLM,
        max_chat_turns: int = 30,  # æ·»åŠ æœ€å¤§å¯¹è¯è½®æ¬¡é™åˆ¶
    ) -> None:
        super().__init__(task_id, model, max_chat_turns)
        self.system_prompt = MODELER_PROMPT

    async def run(self, coordinator_to_modeler: CoordinatorToModeler) -> ModelerToCoder:
        await self.append_chat_history(
            {"role": "system", "content": self.system_prompt}
        )
        await self.append_chat_history(
            {
                "role": "user",
                "content": json.dumps(coordinator_to_modeler.questions),
            }
        )

        response = await self.model.chat(
            history=self.chat_history,
            agent_name=self.__class__.__name__,
        )

        json_str = response.choices[0].message.content

        json_str = json_str.replace("```json", "").replace("```", "").strip()

        if not json_str:
            raise ValueError("è¿”å›çš„ JSON å­—ç¬¦ä¸²ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥å†…å®¹ã€‚")
        try:
            questions_solution = json.loads(json_str)
            ic(questions_solution)
            return ModelerToCoder(questions_solution=questions_solution)
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON è§£æé”™è¯¯: {e}")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\writer_agent.py çš„å†…å®¹:
================================================================================
from app.core.agents.agent import Agent
from app.core.llm.llm import LLM
from app.core.prompts import get_writer_prompt
from app.schemas.enums import CompTemplate, FormatOutPut
from app.tools.openalex_scholar import OpenAlexScholar
from app.utils.log_util import logger
from app.services.redis_manager import redis_manager
from app.schemas.response import SystemMessage, WriterMessage
import json
from app.core.functions import writer_tools
from icecream import ic
from app.schemas.A2A import WriterResponse


# é•¿æ–‡æœ¬
# TODO: å¹¶è¡Œ parallel
# TODO: è·å–å½“å‰æ–‡ä»¶ä¸‹çš„æ–‡ä»¶
# TODO: å¼•ç”¨cites tool
class WriterAgent(Agent):  # åŒæ ·ç»§æ‰¿è‡ªAgentç±»
    def __init__(
        self,
        task_id: str,
        model: LLM,
        max_chat_turns: int = 10,  # æ·»åŠ æœ€å¤§å¯¹è¯è½®æ¬¡é™åˆ¶
        comp_template: CompTemplate = CompTemplate,
        format_output: FormatOutPut = FormatOutPut.Markdown,
        scholar: OpenAlexScholar = None,
        max_memory: int = 25,  # æ·»åŠ æœ€å¤§è®°å¿†è½®æ¬¡
    ) -> None:
        super().__init__(task_id, model, max_chat_turns, max_memory)
        self.format_out_put = format_output
        self.comp_template = comp_template
        self.scholar = scholar
        self.is_first_run = True
        self.system_prompt = get_writer_prompt(format_output)
        self.available_images: list[str] = []

    async def run(
        self,
        prompt: str,
        available_images: list[str] = None,
        sub_title: str = None,
    ) -> WriterResponse:
        """
        æ‰§è¡Œå†™ä½œä»»åŠ¡
        Args:
            prompt: å†™ä½œæç¤º
            available_images: å¯ç”¨çš„å›¾ç‰‡ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼ˆå¦‚ 20250420-173744-9f87792c/ç¼–å·_åˆ†å¸ƒ.pngï¼‰
            sub_title: å­ä»»åŠ¡æ ‡é¢˜
        """
        logger.info(f"subtitleæ˜¯:{sub_title}")

        if self.is_first_run:
            self.is_first_run = False
            await self.append_chat_history(
                {"role": "system", "content": self.system_prompt}
            )

        if available_images:
            self.available_images = available_images
            # æ‹¼æ¥æˆå®Œæ•´URL
            image_list = ",".join(available_images)
            image_prompt = f"\nå¯ç”¨çš„å›¾ç‰‡é“¾æ¥åˆ—è¡¨ï¼š\n{image_list}\nè¯·åœ¨å†™ä½œæ—¶é€‚å½“å¼•ç”¨è¿™äº›å›¾ç‰‡é“¾æ¥ã€‚"
            logger.info(f"image_promptæ˜¯:{image_prompt}")
            prompt = prompt + image_prompt

        logger.info(f"{self.__class__.__name__}:å¼€å§‹:æ‰§è¡Œå¯¹è¯")
        self.current_chat_turns += 1  # é‡ç½®å¯¹è¯è½®æ¬¡è®¡æ•°å™¨

        await self.append_chat_history({"role": "user", "content": prompt})

        # è·å–å†å²æ¶ˆæ¯ç”¨äºæœ¬æ¬¡å¯¹è¯
        response = await self.model.chat(
            history=self.chat_history,
            tools=writer_tools,
            tool_choice="auto",
            agent_name=self.__class__.__name__,
            sub_title=sub_title,
        )

        footnotes = []

        if (
            hasattr(response.choices[0].message, "tool_calls")
            and response.choices[0].message.tool_calls
        ):
            logger.info("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
            tool_call = response.choices[0].message.tool_calls[0]
            tool_id = tool_call.id
            if tool_call.function.name == "search_papers":
                logger.info("è°ƒç”¨å·¥å…·: search_papers")
                await redis_manager.publish_message(
                    self.task_id,
                    SystemMessage(content=f"å†™ä½œæ‰‹è°ƒç”¨{tool_call.function.name}å·¥å…·"),
                )

                query = json.loads(tool_call.function.arguments)["query"]

                await redis_manager.publish_message(
                    self.task_id,
                    WriterMessage(
                        input={"query": query},
                    ),
                )

                # æ›´æ–°å¯¹è¯å†å² - æ·»åŠ åŠ©æ‰‹çš„å“åº”
                await self.append_chat_history(response.choices[0].message.model_dump())
                ic(response.choices[0].message.model_dump())

                try:
                    papers = await self.scholar.search_papers(query)
                except Exception as e:
                    error_msg = f"æœç´¢æ–‡çŒ®å¤±è´¥: {str(e)}"
                    logger.error(error_msg)
                    return WriterResponse(
                        response_content=error_msg, footnotes=footnotes
                    )
                # TODO: pass to frontend
                papers_str = self.scholar.papers_to_str(papers)
                logger.info(f"æœç´¢æ–‡çŒ®ç»“æœ\n{papers_str}")
                await self.append_chat_history(
                    {
                        "role": "tool",
                        "content": papers_str,
                        "tool_call_id": tool_id,
                        "name": "search_papers",
                    }
                )
                next_response = await self.model.chat(
                    history=self.chat_history,
                    tools=writer_tools,
                    tool_choice="auto",
                    agent_name=self.__class__.__name__,
                    sub_title=sub_title,
                )
                response_content = next_response.choices[0].message.content
        else:
            response_content = response.choices[0].message.content
        self.chat_history.append({"role": "assistant", "content": response_content})
        logger.info(f"{self.__class__.__name__}:å®Œæˆ:æ‰§è¡Œå¯¹è¯")
        return WriterResponse(response_content=response_content, footnotes=footnotes)

    async def summarize(self) -> str:
        """
        æ€»ç»“å¯¹è¯å†…å®¹
        """
        try:
            await self.append_chat_history(
                {"role": "user", "content": "è¯·ç®€å•æ€»ç»“ä»¥ä¸Šå®Œæˆä»€ä¹ˆä»»åŠ¡å–å¾—ä»€ä¹ˆç»“æœ:"}
            )
            # è·å–å†å²æ¶ˆæ¯ç”¨äºæœ¬æ¬¡å¯¹è¯
            response = await self.model.chat(
                history=self.chat_history, agent_name=self.__class__.__name__
            )
            await self.append_chat_history(
                {"role": "assistant", "content": response.choices[0].message.content}
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")
            # è¿”å›ä¸€ä¸ªåŸºç¡€æ€»ç»“ï¼Œé¿å…å®Œå…¨å¤±è´¥
            return "ç”±äºç½‘ç»œåŸå› æ— æ³•ç”Ÿæˆè¯¦ç»†æ€»ç»“ï¼Œä½†å·²å®Œæˆä¸»è¦ä»»åŠ¡å¤„ç†ã€‚"


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\agents\__init__.py çš„å†…å®¹:
================================================================================
from .coder_agent import CoderAgent
from .writer_agent import WriterAgent
from .coordinator_agent import CoordinatorAgent
from .modeler_agent import ModelerAgent

__all__ = [
    "CoderAgent",
    "WriterAgent",
    "CoordinatorAgent",
    "ModelerAgent",
]


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\llm\llm.py çš„å†…å®¹:
================================================================================
import json
from app.utils.common_utils import transform_link, split_footnotes
from app.utils.log_util import logger
import time
from app.schemas.response import (
    CoderMessage,
    WriterMessage,
    ModelerMessage,
    SystemMessage,
    CoordinatorMessage,
)
from app.services.redis_manager import redis_manager
from litellm import acompletion
import litellm
from app.schemas.enums import AgentType
from app.utils.track import agent_metrics
from icecream import ic

litellm.callbacks = [agent_metrics]


class LLM:
    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str,
        task_id: str,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.chat_count = 0
        self.max_tokens: int | None = None  # æ·»åŠ æœ€å¤§tokenæ•°é™åˆ¶
        self.task_id = task_id

    async def chat(
        self,
        history: list = None,
        tools: list = None,
        tool_choice: str = None,
        max_retries: int = 8,  # æ·»åŠ æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: float = 1.0,  # æ·»åŠ é‡è¯•å»¶è¿Ÿ
        top_p: float | None = None,  # æ·»åŠ top_på‚æ•°,
        agent_name: AgentType = AgentType.SYSTEM,  # CoderAgent or WriterAgent
        sub_title: str | None = None,
    ) -> str:
        logger.info(f"subtitleæ˜¯:{sub_title}")

        # éªŒè¯å’Œä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§
        if history:
            history = self._validate_and_fix_tool_calls(history)

        kwargs = {
            "api_key": self.api_key,
            "model": self.model,
            "messages": history,
            "stream": False,
            "top_p": top_p,
            "metadata": {"agent_name": agent_name},
        }

        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = tool_choice

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        if self.base_url:
            kwargs["base_url"] = self.base_url

        # TODO: stream è¾“å‡º
        for attempt in range(max_retries):
            try:
                # completion = self.client.chat.completions.create(**kwargs)
                response = await acompletion(**kwargs)
                logger.info(f"APIè¿”å›: {response}")
                if not response or not hasattr(response, "choices"):
                    raise ValueError("æ— æ•ˆçš„APIå“åº”")
                self.chat_count += 1
                await self.send_message(response, agent_name, sub_title)
                return response
            except (json.JSONDecodeError, litellm.InternalServerError) as e:
                logger.error(f"ç¬¬{attempt + 1}æ¬¡é‡è¯•: {str(e)}")
                if attempt < max_retries - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    continue
                logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                raise  # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

    def _validate_and_fix_tool_calls(self, history: list) -> list:
        """éªŒè¯å¹¶ä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§"""
        if not history:
            return history

        ic(f"ğŸ” å¼€å§‹éªŒè¯å·¥å…·è°ƒç”¨ï¼Œå†å²æ¶ˆæ¯æ•°é‡: {len(history)}")

        # æŸ¥æ‰¾æ‰€æœ‰æœªåŒ¹é…çš„tool_calls
        fixed_history = []
        i = 0

        while i < len(history):
            msg = history[i]

            # å¦‚æœæ˜¯åŒ…å«tool_callsçš„æ¶ˆæ¯
            if isinstance(msg, dict) and "tool_calls" in msg and msg["tool_calls"]:
                ic(f"ğŸ“ å‘ç°tool_callsæ¶ˆæ¯åœ¨ä½ç½® {i}")

                # æ£€æŸ¥æ¯ä¸ªtool_callæ˜¯å¦éƒ½æœ‰å¯¹åº”çš„responseï¼Œåˆ†åˆ«å¤„ç†
                valid_tool_calls = []
                invalid_tool_calls = []

                for tool_call in msg["tool_calls"]:
                    tool_call_id = tool_call.get("id")
                    ic(f"  æ£€æŸ¥tool_call_id: {tool_call_id}")

                    if tool_call_id:
                        # æŸ¥æ‰¾å¯¹åº”çš„toolå“åº”
                        found_response = False
                        for j in range(i + 1, len(history)):
                            if (
                                history[j].get("role") == "tool"
                                and history[j].get("tool_call_id") == tool_call_id
                            ):
                                ic(f"  âœ… æ‰¾åˆ°åŒ¹é…å“åº”åœ¨ä½ç½® {j}")
                                found_response = True
                                break

                        if found_response:
                            valid_tool_calls.append(tool_call)
                        else:
                            ic(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…å“åº”: {tool_call_id}")
                            invalid_tool_calls.append(tool_call)

                # æ ¹æ®æ£€æŸ¥ç»“æœå¤„ç†æ¶ˆæ¯
                if valid_tool_calls:
                    # æœ‰æœ‰æ•ˆçš„tool_callsï¼Œä¿ç•™å®ƒä»¬
                    fixed_msg = msg.copy()
                    fixed_msg["tool_calls"] = valid_tool_calls
                    fixed_history.append(fixed_msg)
                    ic(
                        f"  ğŸ”§ ä¿ç•™ {len(valid_tool_calls)} ä¸ªæœ‰æ•ˆtool_callsï¼Œç§»é™¤ {len(invalid_tool_calls)} ä¸ªæ— æ•ˆçš„"
                    )
                else:
                    # æ²¡æœ‰æœ‰æ•ˆçš„tool_callsï¼Œç§»é™¤tool_callsä½†å¯èƒ½ä¿ç•™å…¶ä»–å†…å®¹
                    cleaned_msg = {k: v for k, v in msg.items() if k != "tool_calls"}
                    if cleaned_msg.get("content"):
                        fixed_history.append(cleaned_msg)
                        ic(f"  ğŸ”§ ç§»é™¤æ‰€æœ‰tool_callsï¼Œä¿ç•™æ¶ˆæ¯å†…å®¹")
                    else:
                        ic(f"  ğŸ—‘ï¸ å®Œå…¨ç§»é™¤ç©ºçš„tool_callsæ¶ˆæ¯")

            # å¦‚æœæ˜¯toolå“åº”æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å­¤ç«‹çš„
            elif isinstance(msg, dict) and msg.get("role") == "tool":
                tool_call_id = msg.get("tool_call_id")
                ic(f"ğŸ”§ æ£€æŸ¥toolå“åº”æ¶ˆæ¯: {tool_call_id}")

                # æŸ¥æ‰¾å¯¹åº”çš„tool_calls
                found_call = False
                for j in range(len(fixed_history)):
                    if fixed_history[j].get("tool_calls") and any(
                        tc.get("id") == tool_call_id
                        for tc in fixed_history[j]["tool_calls"]
                    ):
                        found_call = True
                        break

                if found_call:
                    fixed_history.append(msg)
                    ic(f"  âœ… ä¿ç•™æœ‰æ•ˆçš„toolå“åº”")
                else:
                    ic(f"  ğŸ—‘ï¸ ç§»é™¤å­¤ç«‹çš„toolå“åº”: {tool_call_id}")

            else:
                # æ™®é€šæ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
                fixed_history.append(msg)

            i += 1

        if len(fixed_history) != len(history):
            ic(f"ğŸ”§ ä¿®å¤å®Œæˆ: {len(history)} -> {len(fixed_history)} æ¡æ¶ˆæ¯")
        else:
            ic(f"âœ… éªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®å¤")

        return fixed_history

    async def send_message(self, response, agent_name, sub_title=None):
        logger.info(f"subtitleæ˜¯:{sub_title}")
        content = response.choices[0].message.content

        match agent_name:
            case AgentType.CODER:
                agent_msg: CoderMessage = CoderMessage(content=content)
            case AgentType.WRITER:
                # å¤„ç† Markdown æ ¼å¼çš„å›¾ç‰‡è¯­æ³•
                content, _ = split_footnotes(content)
                content = transform_link(self.task_id, content)
                agent_msg: WriterMessage = WriterMessage(
                    content=content,
                    sub_title=sub_title,
                )
            case AgentType.MODELER:
                agent_msg: ModelerMessage = ModelerMessage(content=content)
            case AgentType.SYSTEM:
                agent_msg: SystemMessage = SystemMessage(content=content)
            case AgentType.COORDINATOR:
                agent_msg: CoordinatorMessage = CoordinatorMessage(content=content)
            case _:
                raise ValueError(f"ä¸æ”¯æŒçš„agentç±»å‹: {agent_name}")

        await redis_manager.publish_message(
            self.task_id,
            agent_msg,
        )


# class DeepSeekModel(LLM):
#     def __init__(
#         self,
#         api_key: str,
#         model: str,
#         base_url: str,
#         task_id: str,
#     ):
#         super().__init__(api_key, model, base_url, task_id)
# self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)


async def simple_chat(model: LLM, history: list) -> str:
    """
    Description of the function.

    Args:
        model (LLM): æ¨¡å‹
        history (list): æ„é€ å¥½çš„å†å²è®°å½•ï¼ˆåŒ…å«system_prompt,user_promptï¼‰

    Returns:
        return_type: Description of the return value.
    """
    kwargs = {
        "api_key": model.api_key,
        "model": model.model,
        "messages": history,
        "stream": False,
    }

    if model.base_url:
        kwargs["base_url"] = model.base_url

    response = await acompletion(**kwargs)

    return response.choices[0].message.content


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\llm\llm_factory.py çš„å†…å®¹:
================================================================================
from app.config.setting import settings
from app.core.llm.llm import LLM


class LLMFactory:
    task_id: str

    def __init__(self, task_id: str) -> None:
        self.task_id = task_id

    def get_all_llms(self) -> tuple[LLM, LLM, LLM, LLM]:
        coordinator_llm = LLM(
            api_key=settings.COORDINATOR_API_KEY,
            model=settings.COORDINATOR_MODEL,
            base_url=settings.COORDINATOR_BASE_URL,
            task_id=self.task_id,
        )

        modeler_llm = LLM(
            api_key=settings.MODELER_API_KEY,
            model=settings.MODELER_MODEL,
            base_url=settings.MODELER_BASE_URL,
            task_id=self.task_id,
        )

        coder_llm = LLM(
            api_key=settings.CODER_API_KEY,
            model=settings.CODER_MODEL,
            base_url=settings.CODER_BASE_URL,
            task_id=self.task_id,
        )

        writer_llm = LLM(
            api_key=settings.WRITER_API_KEY,
            model=settings.WRITER_MODEL,
            base_url=settings.WRITER_BASE_URL,
            task_id=self.task_id,
        )

        return coordinator_llm, modeler_llm, coder_llm, writer_llm


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\core\llm\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\models\user_output.py çš„å†…å®¹:
================================================================================
import os
import re
from app.utils.data_recorder import DataRecorder
from app.schemas.A2A import WriterResponse
import json
import uuid


class UserOutput:
    def __init__(
        self, work_dir: str, ques_count: int, data_recorder: DataRecorder | None = None
    ):
        self.work_dir = work_dir
        self.res: dict[str, dict] = {
            # "eda": {
            #     "response_content": "",
            #     "footnotes": "",
            # },
            # "ques1": {
            #     "response_content": "",
            #     "footnotes": "",
            # },
        }
        self.data_recorder = data_recorder
        self.cost_time = 0.0
        self.initialized = True
        self.ques_count: int = ques_count
        self.footnotes = {}
        self._init_seq()

    def _init_seq(self):
        # åŠ¨æ€é¡ºåºè·å–æ‹¼æ¥res valueï¼Œæ­£ç¡®æ‹¼æ¥é¡ºåº
        ques_str = [f"ques{i}" for i in range(1, self.ques_count + 1)]

        # ä¿®æ”¹ï¼šè°ƒæ•´ç« èŠ‚é¡ºåºï¼Œç¡®ä¿ç¬¦åˆè®ºæ–‡ç»“æ„
        self.seq = [
            "firstPage",  # æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯
            "RepeatQues",  # ä¸€ã€é—®é¢˜é‡è¿°
            "analysisQues",  # äºŒã€é—®é¢˜åˆ†æ
            "modelAssumption",  # ä¸‰ã€æ¨¡å‹å‡è®¾
            "symbol",  # å››ã€ç¬¦å·è¯´æ˜å’Œæ•°æ®é¢„å¤„ç†
            "eda",  # å››ã€æ•°æ®é¢„å¤„ç†ï¼ˆEDAéƒ¨åˆ†ï¼‰
            *ques_str,  # äº”ã€æ¨¡å‹çš„å»ºç«‹ä¸æ±‚è§£ï¼ˆé—®é¢˜1ã€2...ï¼‰
            "sensitivity_analysis",  # å…­ã€æ¨¡å‹çš„åˆ†æä¸æ£€éªŒ
            "judge",  # ä¸ƒã€æ¨¡å‹çš„è¯„ä»·ã€æ”¹è¿›ä¸æ¨å¹¿
        ]

    def set_res(self, key: str, writer_response: WriterResponse):
        self.res[key] = {
            "response_content": writer_response.response_content,
            "footnotes": writer_response.footnotes,
        }

    def get_res(self):
        return self.res

    def get_model_build_solve(self) -> str:
        """è·å–æ¨¡å‹æ±‚è§£"""
        model_build_solve = ",".join(
            f"{key}-{value}"
            for key, value in self.res.items()
            if key.startswith("ques") and key != "ques_count"
        )

        return model_build_solve

    def replace_references_with_uuid(self, text: str) -> str:
        # åŒ¹é…å¼•ç”¨å†…å®¹ï¼Œæ ¼å¼ä¸º {[^æ•°å­—]: å¼•ç”¨å†…å®¹}
        # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…å¤§æ‹¬å·åŒ…è£¹çš„å¼•ç”¨æ ¼å¼
        references = re.findall(r"\{\[\^(\d+)\]:\s*(.*?)\}", text, re.DOTALL)

        for ref_num, ref_content in references:
            # æ¸…ç†å¼•ç”¨å†…å®¹ï¼Œå»é™¤æœ«å°¾çš„ç©ºæ ¼å’Œç‚¹å·
            ref_content = ref_content.strip().rstrip(".")

            # æ£€æŸ¥å½“å‰å¼•ç”¨å†…å®¹æ˜¯å¦å·²ç»å­˜åœ¨äºfootnotesä¸­
            existing_uuid = None
            for uuid_key, footnote_data in self.footnotes.items():
                if footnote_data["content"] == ref_content:
                    existing_uuid = uuid_key
                    break

            if existing_uuid:
                # å¦‚æœå·²å­˜åœ¨ï¼Œä½¿ç”¨ç°æœ‰çš„UUID
                text = re.sub(
                    rf"\{{\[\^{ref_num}\]:.*?\}}",
                    f"[{existing_uuid}]",
                    text,
                    flags=re.DOTALL,
                )
            else:
                # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„UUIDå’Œfootnoteæ¡ç›®
                new_uuid = str(uuid.uuid4())
                self.footnotes[new_uuid] = {
                    "content": ref_content,
                }
                text = re.sub(
                    rf"\{{\[\^{ref_num}\]:.*?\}}",
                    f"[{new_uuid}]",
                    text,
                    flags=re.DOTALL,
                )

        return text

    def sort_text_with_footnotes(self, replace_res: dict) -> dict:
        sort_res = {}
        ref_index = 1

        for seq_key in self.seq:
            text = replace_res[seq_key]["response_content"]
            # æ‰¾åˆ°[uuid]
            uuid_list = re.findall(r"\[([a-f0-9-]{36})\]", text)
            for uid in uuid_list:
                text = text.replace(f"[{uid}]", f"[^{ref_index}]")
                if self.footnotes[uid].get("number") is None:
                    self.footnotes[uid]["number"] = ref_index

                ref_index += 1
            sort_res[seq_key] = {
                "response_content": text,
            }

        return sort_res

    def append_footnotes_to_text(self, text: str) -> str:
        text += "\n\n ## å‚è€ƒæ–‡çŒ®"
        # å°†è„šæ³¨è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰ number æ’åº
        sorted_footnotes = sorted(self.footnotes.items(), key=lambda x: x[1]["number"])
        for _, footnote in sorted_footnotes:
            text += f"\n\n[^{footnote['number']}]: {footnote['content']}"
        return text

    def get_result_to_save(self) -> str:
        replace_res = {}

        for key, value in self.res.items():
            new_text = self.replace_references_with_uuid(value["response_content"])
            replace_res[key] = {
                "response_content": new_text,
            }

        sort_res = self.sort_text_with_footnotes(replace_res)

        full_res_1 = "\n\n".join(
            [sort_res[key]["response_content"] for key in self.seq]
        )

        full_res = self.append_footnotes_to_text(full_res_1)
        return full_res

    def save_result(
        self,
    ):
        with open(os.path.join(self.work_dir, "res.json"), "w", encoding="utf-8") as f:
            json.dump(self.res, f, ensure_ascii=False, indent=4)

        res_path = os.path.join(self.work_dir, "res.md")
        with open(res_path, "w", encoding="utf-8") as f:
            f.write(self.get_result_to_save())


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\models\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\routers\common_router.py çš„å†…å®¹:
================================================================================
from fastapi import APIRouter
from app.config.setting import settings
from app.utils.common_utils import get_config_template
from app.schemas.enums import CompTemplate

router = APIRouter()


@router.get("/")
async def root():
    return {"message": "Hello World"}


@router.get("/config")
async def config():
    return {
        "environment": settings.ENV,
        "deepseek_model": settings.DEEPSEEK_MODEL,
        "deepseek_base_url": settings.DEEPSEEK_BASE_URL,
        "max_chat_turns": settings.MAX_CHAT_TURNS,
        "max_retries": settings.MAX_RETRIES,
        "CORS_ALLOW_ORIGINS": settings.CORS_ALLOW_ORIGINS,
    }


@router.get("/writer_seque")
async def get_writer_seque():
    # è¿”å›è®ºæ–‡é¡ºåº
    config_template: dict = get_config_template(CompTemplate.CHINA)
    return list(config_template.keys())


@router.get("/track")
async def track(task_id: str):
    # è·å–ä»»åŠ¡çš„tokenä½¿ç”¨æƒ…å†µ

    pass


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\routers\files_router.py çš„å†…å®¹:
================================================================================
from fastapi import APIRouter
from app.utils.common_utils import get_current_files, get_work_dir
import os
import subprocess
from icecream import ic
from fastapi import HTTPException

router = APIRouter()


@router.get("/files")
async def get_files(task_id: str):
    work_dir = get_work_dir(task_id)
    files = get_current_files(work_dir, "all")

    return {"files": files}


@router.get("/open_folder")
async def open_folder(task_id: str):
    ic(task_id)
    # æ‰“å¼€å·¥ä½œç›®å½•
    work_dir = get_work_dir(task_id)

    # æ‰“å¼€å·¥ä½œç›®å½•
    if os.name == "nt":
        subprocess.run(["explorer", work_dir])
    elif os.name == "posix":
        subprocess.run(["open", work_dir])
    else:
        raise HTTPException(status_code=500, detail=f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: {os.name}")

    return {"message": "æ‰“å¼€å·¥ä½œç›®å½•æˆåŠŸ", "work_dir": work_dir}


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\routers\modeling_router.py çš„å†…å®¹:
================================================================================
from fastapi import APIRouter, BackgroundTasks, File, Form, UploadFile
from app.core.workflow import MathModelWorkFlow
from app.schemas.enums import CompTemplate, FormatOutPut
from app.utils.log_util import logger
from app.services.redis_manager import redis_manager
from app.schemas.request import Problem
from app.schemas.response import SystemMessage
from app.utils.common_utils import (
    create_task_id,
    create_work_dir,
    get_current_files,
    md_2_docx,
)
import os
import asyncio
from fastapi import HTTPException
from icecream import ic
from app.schemas.request import ExampleRequest

router = APIRouter()


@router.post("/example")
async def exampleModeling(
    example_request: ExampleRequest,
    background_tasks: BackgroundTasks,
):
    task_id = create_task_id()
    work_dir = create_work_dir(task_id)
    example_dir = os.path.join("app", "example", "example", example_request.source)
    ic(example_dir)
    with open(os.path.join(example_dir, "questions.txt"), "r", encoding="utf-8") as f:
        ques_all = f.read()

    current_files = get_current_files(example_dir, "data")
    for file in current_files:
        src_file = os.path.join(example_dir, file)
        dst_file = os.path.join(work_dir, file)
        with open(src_file, "rb") as src, open(dst_file, "wb") as dst:
            dst.write(src.read())
    # å­˜å‚¨ä»»åŠ¡ID
    await redis_manager.set(f"task_id:{task_id}", task_id)

    logger.info(f"Adding background task for task_id: {task_id}")
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°æ‰§è¡Œ
    background_tasks.add_task(
        run_modeling_task_async,
        task_id,
        ques_all,
        CompTemplate.CHINA,
        FormatOutPut.Markdown,
    )
    return {"task_id": task_id, "status": "processing"}


@router.post("/modeling")
async def modeling(
    background_tasks: BackgroundTasks,
    ques_all: str = Form(...),  # ä»è¡¨å•è·å–
    comp_template: CompTemplate = Form(...),  # ä»è¡¨å•è·å–
    format_output: FormatOutPut = Form(...),  # ä»è¡¨å•è·å–
    files: list[UploadFile] = File(default=None),
):
    task_id = create_task_id()
    work_dir = create_work_dir(task_id)

    # å¦‚æœæœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œä¿å­˜æ–‡ä»¶
    if files:
        logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå·¥ä½œç›®å½•: {work_dir}")
        for file in files:
            try:
                data_file_path = os.path.join(work_dir, file.filename)
                logger.info(f"ä¿å­˜æ–‡ä»¶: {file.filename} -> {data_file_path}")

                # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
                if not file.filename:
                    logger.warning("è·³è¿‡ç©ºæ–‡ä»¶å")
                    continue

                content = await file.read()
                if not content:
                    logger.warning(f"æ–‡ä»¶ {file.filename} å†…å®¹ä¸ºç©º")
                    continue

                with open(data_file_path, "wb") as f:
                    f.write(content)
                logger.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶: {data_file_path}")

            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡ä»¶ {file.filename} å¤±è´¥: {str(e)}")
                raise HTTPException(
                    status_code=500, detail=f"ä¿å­˜æ–‡ä»¶ {file.filename} å¤±è´¥: {str(e)}"
                )
    else:
        logger.warning("æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")

    # å­˜å‚¨ä»»åŠ¡ID
    await redis_manager.set(f"task_id:{task_id}", task_id)

    logger.info(f"Adding background task for task_id: {task_id}")
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°æ‰§è¡Œ
    background_tasks.add_task(
        run_modeling_task_async, task_id, ques_all, comp_template, format_output
    )
    return {"task_id": task_id, "status": "processing"}


async def run_modeling_task_async(
    task_id: str,
    ques_all: str,
    comp_template: CompTemplate,
    format_output: FormatOutPut,
):
    logger.info(f"run modeling task for task_id: {task_id}")

    problem = Problem(
        task_id=task_id,
        ques_all=ques_all,
        comp_template=comp_template,
        format_output=format_output,
    )

    # å‘é€ä»»åŠ¡å¼€å§‹çŠ¶æ€
    await redis_manager.publish_message(
        task_id,
        SystemMessage(content="ä»»åŠ¡å¼€å§‹å¤„ç†"),
    )

    # ç»™ä¸€ä¸ªçŸ­æš‚çš„å»¶è¿Ÿï¼Œç¡®ä¿ WebSocket æœ‰æœºä¼šè¿æ¥
    await asyncio.sleep(1)

    # åˆ›å»ºä»»åŠ¡å¹¶ç­‰å¾…å®ƒå®Œæˆ
    task = asyncio.create_task(MathModelWorkFlow().execute(problem))
    # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆæ¯”å¦‚ 60 åˆ†é’Ÿï¼‰
    await asyncio.wait_for(task, timeout=3600)

    # å‘é€ä»»åŠ¡å®ŒæˆçŠ¶æ€
    await redis_manager.publish_message(
        task_id,
        SystemMessage(content="ä»»åŠ¡å¤„ç†å®Œæˆ", type="success"),
    )
    # è½¬æ¢mdä¸ºdocx
    md_2_docx(task_id)


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\routers\ws_router.py çš„å†…å®¹:
================================================================================
from fastapi import WebSocket, WebSocketDisconnect, APIRouter
from app.services.redis_manager import redis_manager
from app.schemas.response import SystemMessage
import asyncio
from app.services.ws_manager import ws_manager
import json

router = APIRouter()


@router.websocket("/task/{task_id}")
async def websocket_endpoint(websocket: WebSocket, task_id: str):
    print(f"WebSocket å°è¯•è¿æ¥ task_id: {task_id}")

    redis_async_client = await redis_manager.get_client()
    if not await redis_async_client.exists(f"task_id:{task_id}"):
        print(f"Task not found: {task_id}")
        await websocket.close(code=1008, reason="Task not found")
        return
    print(f"WebSocket connected for task: {task_id}")

    # å»ºç«‹ WebSocket è¿æ¥
    await ws_manager.connect(websocket)
    websocket.timeout = 500
    print(f"WebSocket connection status: {websocket.client}")

    # è®¢é˜… Redis é¢‘é“
    pubsub = await redis_manager.subscribe_to_task(task_id)
    print(f"Subscribed to Redis channel: task:{task_id}:messages")

    await redis_manager.publish_message(
        task_id,
        SystemMessage(content="ä»»åŠ¡å¼€å§‹å¤„ç†"),
    )

    try:
        while True:
            try:
                msg = await pubsub.get_message(ignore_subscribe_messages=True)
                if msg:
                    print(f"Received message: {msg}")
                    try:
                        msg_dict = json.loads(msg["data"])
                        await ws_manager.send_personal_message_json(msg_dict, websocket)
                        print(f"Sent message to WebSocket: {msg_dict}")
                    except Exception as e:
                        print(f"Error parsing message: {e}")
                        await ws_manager.send_personal_message_json(
                            {"error": str(e)}, websocket
                        )
                await asyncio.sleep(0.1)

            except WebSocketDisconnect:
                print("WebSocket disconnected")
                break
            except Exception as e:
                print(f"Error in websocket loop: {e}")
                await asyncio.sleep(1)
                continue

    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        await pubsub.unsubscribe(f"task:{task_id}:messages")
        ws_manager.disconnect(websocket)
        print(f"WebSocket connection closed for task: {task_id}")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\routers\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\A2A.py çš„å†…å®¹:
================================================================================
from pydantic import BaseModel
from typing import Any


class CoordinatorToModeler(BaseModel):
    questions: dict
    ques_count: int


class ModelerToCoder(BaseModel):
    questions_solution: dict[str, str]


class CoderToWriter(BaseModel):
    code_response: str | None = None
    code_output: str | None = None
    created_images: list[str] | None = None


class WriterResponse(BaseModel):
    response_content: Any
    footnotes: list[tuple[str, str]] | None = None


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\base.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\enums.py çš„å†…å®¹:
================================================================================
from enum import Enum


class CompTemplate(str, Enum):
    CHINA: str = "CHINA"
    AMERICAN: str = "AMERICAN"


class FormatOutPut(str, Enum):
    Markdown: str = "Markdown"
    LaTeX: str = "LaTeX"


class AgentType(str, Enum):
    COORDINATOR = "CoordinatorAgent"
    MODELER = "ModelerAgent"
    CODER = "CoderAgent"
    WRITER = "WriterAgent"
    SYSTEM = "SystemAgent"


class AgentStatus(str, Enum):
    START = "start"
    WORKING = "working"
    DONE = "done"
    ERROR = "error"
    SUCCESS = "success"


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\request.py çš„å†…å®¹:
================================================================================
from pydantic import BaseModel
from app.schemas.enums import CompTemplate, FormatOutPut


class ExampleRequest(BaseModel):
    example_id: str
    source: str


class Problem(BaseModel):
    task_id: str
    ques_all: str = ""
    comp_template: CompTemplate = CompTemplate.CHINA
    format_output: FormatOutPut = FormatOutPut.Markdown

    def model_dump(self, **kwargs):
        data = super().model_dump(**kwargs)
        data["comp_template"] = self.comp_template.value
        data["format_output"] = self.format_output.value
        return data


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\response.py çš„å†…å®¹:
================================================================================
from typing import Literal, Union
from app.schemas.enums import AgentType
from pydantic import BaseModel, Field
from uuid import uuid4


class Message(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid4()))
    msg_type: Literal[
        "system", "agent", "user", "tool"
    ]  # system msg | agent message | user message | tool message
    content: str | None = None


class ToolMessage(Message):
    msg_type: str = "tool"
    tool_name: Literal["execute_code", "search_scholar"]
    input: dict
    output: list


class SystemMessage(Message):
    msg_type: str = "system"
    type: Literal["info", "warning", "success", "error"] = "info"


class UserMessage(Message):
    msg_type: str = "user"


class AgentMessage(Message):
    msg_type: str = "agent"
    agent_type: AgentType  # CoordinatorAgent | ModelerAgent | CoderAgent | WriterAgent


class ModelerMessage(AgentMessage):
    agent_type: AgentType = AgentType.MODELER


class CoordinatorMessage(AgentMessage):
    agent_type: AgentType = AgentType.COORDINATOR


class CodeExecution(BaseModel):
    res_type: Literal["stdout", "stderr", "result", "error"]
    msg: str | None = None


class StdOutModel(CodeExecution):
    res_type: str = "stdout"


class StdErrModel(CodeExecution):
    res_type: str = "stderr"


class ResultModel(CodeExecution):
    res_type: str = "result"
    format: Literal[
        "text",
        "html",
        "markdown",
        "png",
        "jpeg",
        "svg",
        "pdf",
        "latex",
        "json",
        "javascript",
    ]


class ErrorModel(CodeExecution):
    res_type: str = "error"
    name: str
    value: str
    traceback: str


# ä»£ç æ‰§è¡Œç»“æœç±»å‹
OutputItem = Union[StdOutModel, StdErrModel, ResultModel, ErrorModel]


class ScholarMessage(ToolMessage):
    tool_name: str = "search_scholar"
    input: dict | None = None  # query
    output: list[str] | None = None  # cites


class InterpreterMessage(ToolMessage):
    tool_name: str = "execute_code"
    input: dict | None = None  # code
    output: list[OutputItem] | None = None  # code_results


# 1. åªå¸¦ code
# 2. åªå¸¦ code result
class CoderMessage(AgentMessage):
    agent_type: AgentType = AgentType.CODER


class WriterMessage(AgentMessage):
    agent_type: AgentType = AgentType.WRITER
    sub_title: str | None = None


# æ‰€æœ‰å¯èƒ½çš„æ¶ˆæ¯ç±»å‹
MessageType = Union[
    SystemMessage,
    UserMessage,
    ModelerMessage,
    CoderMessage,
    WriterMessage,
    CoordinatorMessage,
]


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\tool_result.py çš„å†…å®¹:
================================================================================
from pydantic import BaseModel
from typing import Any, Optional


class ToolResult(BaseModel):
    success: bool
    message: Optional[str] = None
    data: Optional[Any] = None


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\schemas\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\services\redis_manager.py çš„å†…å®¹:
================================================================================
import redis.asyncio as aioredis
from typing import Optional
import json
from pathlib import Path
from app.config.setting import settings
from app.schemas.response import Message
from app.utils.log_util import logger


class RedisManager:
    def __init__(self):
        self.redis_url = settings.REDIS_URL
        self._client: Optional[aioredis.Redis] = None
        # åˆ›å»ºæ¶ˆæ¯å­˜å‚¨ç›®å½•
        self.messages_dir = Path("logs/messages")
        self.messages_dir.mkdir(parents=True, exist_ok=True)

    async def get_client(self) -> aioredis.Redis:
        if self._client is None:
            self._client = aioredis.Redis.from_url(
                self.redis_url,
                decode_responses=True,
                max_connections=settings.REDIS_MAX_CONNECTIONS,
            )
        logger.info(f"Redis è¿æ¥å»ºç«‹æˆåŠŸ: {self.redis_url}")
        return self._client

    async def set(self, key: str, value: str):
        """è®¾ç½®Redisé”®å€¼å¯¹"""
        client = await self.get_client()
        await client.set(key, value)
        await client.expire(key, 36000)

    async def _save_message_to_file(self, task_id: str, message: Message):
        """å°†æ¶ˆæ¯ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼ŒåŒä¸€ä»»åŠ¡çš„æ¶ˆæ¯ä¿å­˜åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.messages_dir.mkdir(exist_ok=True)

            # ä½¿ç”¨ä»»åŠ¡IDä½œä¸ºæ–‡ä»¶å
            file_path = self.messages_dir / f"{task_id}.json"

            # è¯»å–ç°æœ‰æ¶ˆæ¯ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
            messages = []
            if file_path.exists():
                with open(file_path, "r", encoding="utf-8") as f:
                    messages = json.load(f)

            # æ·»åŠ æ–°æ¶ˆæ¯
            message_data = message.model_dump()
            messages.append(message_data)

            # ä¿å­˜æ‰€æœ‰æ¶ˆæ¯åˆ°æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(messages, f, ensure_ascii=False, indent=2)

            logger.debug(f"æ¶ˆæ¯å·²è¿½åŠ åˆ°æ–‡ä»¶: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿ä¸»æµç¨‹ä¸å—å½±å“

    async def publish_message(self, task_id: str, message: Message):
        """å‘å¸ƒæ¶ˆæ¯åˆ°ç‰¹å®šä»»åŠ¡çš„é¢‘é“å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        client = await self.get_client()
        channel = f"task:{task_id}:messages"
        try:
            message_json = message.model_dump_json()
            await client.publish(channel, message_json)
            logger.debug(
                f"æ¶ˆæ¯å·²å‘å¸ƒåˆ°é¢‘é“ {channel}:mes_type:{message.msg_type}:msg_content:{message.content}"
            )
            # ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶
            await self._save_message_to_file(task_id, message)
        except Exception as e:
            logger.error(f"å‘å¸ƒæ¶ˆæ¯å¤±è´¥: {str(e)}")
            raise

    async def subscribe_to_task(self, task_id: str):
        """è®¢é˜…ç‰¹å®šä»»åŠ¡çš„æ¶ˆæ¯"""
        client = await self.get_client()
        pubsub = client.pubsub()
        await pubsub.subscribe(f"task:{task_id}:messages")
        return pubsub

    async def close(self):
        """å…³é—­Redisè¿æ¥"""
        if self._client:
            await self._client.close()
            self._client = None


redis_manager = RedisManager()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\services\ws_manager.py çš„å†…å®¹:
================================================================================
from fastapi import WebSocket


class WebSocketManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def send_personal_message_json(self, message: dict, websocket: WebSocket):
        await websocket.send_json(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)


ws_manager = WebSocketManager()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tests\get_config_template.py çš„å†…å®¹:
================================================================================
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.schemas.enums import CompTemplate


def test_get_config_template():
    from app.utils.common_utils import get_config_template

    comp_template = CompTemplate.CHINA
    config_template = get_config_template(comp_template)
    print(config_template)


if __name__ == "__main__":
    test_get_config_template()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tests\test_common_utils.py çš„å†…å®¹:
================================================================================
import unittest

from app.utils.common_utils import split_footnotes


class TestCommonUtils(unittest.TestCase):
    def test_split_footnotes(self):
        text = "Example[^1]\n\n[^1]: Footnote content"
        main, notes = split_footnotes(text)
        self.assertEqual(main, "Example")
        self.assertEqual(notes, [("1", "Footnote content")])


if __name__ == "__main__":
    unittest.main()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tests\test_e2b.py çš„å†…å®¹:
================================================================================
import os
import asyncio
import unittest


from dotenv import load_dotenv

from app.tools.e2b_interpreter import E2BCodeInterpreter
from app.utils.common_utils import create_work_dir

try:
    from dotenv import load_dotenv
except ModuleNotFoundError:  # Fallback if python-dotenv is not installed
    def load_dotenv(*args, **kwargs):
        return None

try:
    from app.tools.e2b_interpreter import E2BCodeInterpreter
except ModuleNotFoundError:
    E2BCodeInterpreter = None
from app.utils.common_utils import create_task_id, create_work_dir



class TestE2BCodeInterpreter(unittest.TestCase):
    def setUp(self):
        load_dotenv()

        if E2BCodeInterpreter is None:
            self.skipTest("e2b_code_interpreter not available")
        _, dirs = create_work_dir("20250312-104132-d3625cab")
        notebook = NotebookSerializer(dirs["jupyter"])

        self.code_interpreter = E2BCodeInterpreter(
            self.task_id, self.work_dir, notebook
        )

    def test_execute_code(self):
        if not os.getenv("E2B_API_KEY"):
            self.skipTest("E2B_API_KEY not set")

        code = """
import matplotlib.pyplot as plt
import numpy as np

# ç”Ÿæˆæ•°æ®
x = np.linspace(0, 2 * np.pi, 100)  # xä»0åˆ°2Ï€ï¼Œç”Ÿæˆ100ä¸ªç‚¹
y = np.sin(x)                       # è®¡ç®—å¯¹åº”çš„sin(x)å€¼

# ç»˜å›¾
plt.figure(figsize=(8, 4))          # è®¾ç½®ç”»å¸ƒå¤§å°
plt.plot(x, y, label='y = sin(x)')  # ç»˜åˆ¶æ›²çº¿ï¼Œå¹¶æ·»åŠ å›¾ä¾‹

# æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
plt.title("Simple Sine Function")
plt.xlabel("x")
plt.ylabel("y")

# æ·»åŠ ç½‘æ ¼å’Œå›¾ä¾‹
plt.grid(True)
plt.legend()

# æ˜¾ç¤ºå›¾åƒ
plt.show()
"""
        asyncio.run(self.code_interpreter.initialize())
        asyncio.run(self.code_interpreter.execute_code(code))



================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tests\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\base.py çš„å†…å®¹:
================================================================================
from typing import Dict, Any, List, Callable
import inspect
from app.schemas.tool_result import ToolResult


def tool(
    name: str,
    description: str,
    parameters: Dict[str, Dict[str, Any]],
    required: List[str],
) -> Callable:
    """Tool registration decorator

    Args:
        name: Tool name
        description: Tool description
        parameters: Tool parameter definitions
        required: List of required parameters

    Returns:
        Decorator function
    """

    def decorator(func):
        # Create tool schema directly using provided parameters, without automatic extraction
        schema = {
            "type": "function",
            "function": {
                "name": name,
                "description": description,
                "parameters": {
                    "type": "object",
                    "properties": parameters,
                    "required": required,
                },
            },
        }

        # Store tool information
        func._function_name = name
        func._tool_description = description
        func._tool_schema = schema

        return func

    return decorator


class BaseTool:
    """Base tool class, providing common tool calling methods"""

    name: str = ""

    def __init__(self):
        """Initialize base tool class"""
        self._tools_cache = None

    def get_tools(self) -> List[Dict[str, Any]]:
        """Get all registered tools

        Returns:
            List of tools
        """
        if self._tools_cache is not None:
            return self._tools_cache

        tools = []
        for _, method in inspect.getmembers(self, inspect.ismethod):
            if hasattr(method, "_tool_schema"):
                tools.append(method._tool_schema)

        self._tools_cache = tools
        return tools

    def has_function(self, function_name: str) -> bool:
        """Check if specified function exists

        Args:
            function_name: Function name

        Returns:
            Whether the tool exists
        """
        for _, method in inspect.getmembers(self, inspect.ismethod):
            if (
                hasattr(method, "_function_name")
                and method._function_name == function_name
            ):
                return True
        return False

    async def invoke_function(self, function_name: str, **kwargs) -> ToolResult:
        """Invoke specified tool

        Args:
            function_name: Function name
            **kwargs: Parameters

        Returns:
            Invocation result

        Raises:
            ValueError: Raised when tool doesn't exist
        """
        for _, method in inspect.getmembers(self, inspect.ismethod):
            if (
                hasattr(method, "_function_name")
                and method._function_name == function_name
            ):
                return await method(**kwargs)

        raise ValueError(f"Tool '{function_name}' not found")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\base_interpreter.py çš„å†…å®¹:
================================================================================
# base_interpreter.py
import abc
import re
from app.tools.notebook_serializer import NotebookSerializer
from app.services.redis_manager import redis_manager
from app.utils.log_util import logger
from app.schemas.response import (
    OutputItem,
    InterpreterMessage,
)


class BaseCodeInterpreter(abc.ABC):
    def __init__(
        self,
        task_id: str,
        work_dir: str,
        notebook_serializer: NotebookSerializer,
    ):
        self.task_id = task_id
        self.work_dir = work_dir
        self.notebook_serializer = notebook_serializer
        self.section_output: dict[str, dict[str, list[str]]] = {}
        self.last_created_images = set()

    @abc.abstractmethod
    async def initialize(self):
        """åˆå§‹åŒ–è§£é‡Šå™¨ï¼Œå¿…è¦æ—¶ä¸Šä¼ æ–‡ä»¶ã€å¯åŠ¨å†…æ ¸ç­‰"""
        ...

    @abc.abstractmethod
    async def _pre_execute_code(self):
        """æ‰§è¡Œåˆå§‹åŒ–ä»£ç """
        ...

    @abc.abstractmethod
    async def execute_code(self, code: str) -> tuple[str, bool, str]:
        """æ‰§è¡Œä¸€æ®µä»£ç ï¼Œè¿”å› (è¾“å‡ºæ–‡æœ¬, æ˜¯å¦å‡ºé”™, é”™è¯¯ä¿¡æ¯)"""
        ...

    @abc.abstractmethod
    async def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œæ¯”å¦‚å…³é—­æ²™ç®±æˆ–å†…æ ¸"""
        ...

    @abc.abstractmethod
    async def get_created_images(self, section: str) -> list[str]:
        """è·å–å½“å‰ section åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨"""
        ...

    async def _push_to_websocket(self, content_to_display: list[OutputItem] | None):
        logger.info("æ‰§è¡Œç»“æœå·²æ¨é€åˆ°WebSocket")

        agent_msg = InterpreterMessage(
            output=content_to_display,
        )
        logger.debug(f"å‘é€æ¶ˆæ¯: {agent_msg.model_dump_json()}")
        await redis_manager.publish_message(
            self.task_id,
            agent_msg,
        )

    def add_section(self, section_name: str) -> None:
        """ç¡®ä¿æ·»åŠ çš„sectionç»“æ„æ­£ç¡®"""

        if section_name not in self.section_output:
            self.section_output[section_name] = {"content": [], "images": []}

    def add_content(self, section: str, text: str) -> None:
        """å‘æŒ‡å®šsectionæ·»åŠ æ–‡æœ¬å†…å®¹"""
        self.add_section(section)
        self.section_output[section]["content"].append(text)

    def get_code_output(self, section: str) -> str:
        """è·å–æŒ‡å®šsectionçš„ä»£ç è¾“å‡º"""
        return "\n".join(self.section_output[section]["content"])

    def delete_color_control_char(self, string):
        ansi_escape = re.compile(r"(\x9B|\x1B\[)[0-?]*[ -\/]*[@-~]")
        return ansi_escape.sub("", string)

    def _truncate_text(self, text: str, max_length: int = 1000) -> str:
        """æˆªæ–­æ–‡æœ¬ï¼Œä¿ç•™å¼€å¤´å’Œç»“å°¾çš„é‡è¦ä¿¡æ¯"""
        if len(text) <= max_length:
            return text

        half_length = max_length // 2
        return text[:half_length] + "\n... (å†…å®¹å·²æˆªæ–­) ...\n" + text[-half_length:]


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\e2b_interpreter.py çš„å†…å®¹:
================================================================================
import os
from e2b_code_interpreter import AsyncSandbox
from app.schemas.response import (
    ErrorModel,
    OutputItem,
    ResultModel,
    StdErrModel,
    StdOutModel,
    SystemMessage,
)
from app.services.redis_manager import redis_manager
from app.tools.notebook_serializer import NotebookSerializer
from app.utils.log_util import logger
from app.config.setting import settings
import json
from app.tools.base_interpreter import BaseCodeInterpreter


class E2BCodeInterpreter(BaseCodeInterpreter):
    def __init__(
        self,
        task_id: str,
        work_dir: str,
        notebook_serializer: NotebookSerializer,
    ):
        super().__init__(task_id, work_dir, notebook_serializer)
        self.sbx = None

    @classmethod
    async def create(
        cls,
        task_id: str,
        work_dir: str,
        notebook_serializer: NotebookSerializer,
    ) -> "E2BCodeInterpreter":
        """åˆ›å»ºå¹¶åˆå§‹åŒ– E2BCodeInterpreter å®ä¾‹"""
        instance = cls(task_id, work_dir, notebook_serializer)
        return instance

    async def initialize(self, timeout: int = 3000):
        """å¼‚æ­¥åˆå§‹åŒ–æ²™ç®±ç¯å¢ƒ"""
        try:
            self.sbx = await AsyncSandbox.create(
                api_key=settings.E2B_API_KEY, timeout=timeout
            )
            logger.info("æ²™ç®±ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            await self._pre_execute_code()
            await self._upload_all_files()
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ²™ç®±ç¯å¢ƒå¤±è´¥: {str(e)}")
            raise

    async def _upload_all_files(self):
        """ä¸Šä¼ å·¥ä½œç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶åˆ°æ²™ç®±"""
        try:
            logger.info(f"å¼€å§‹ä¸Šä¼ æ–‡ä»¶ï¼Œå·¥ä½œç›®å½•: {self.work_dir}")
            if not os.path.exists(self.work_dir):
                logger.error(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {self.work_dir}")
                raise FileNotFoundError(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {self.work_dir}")

            files = [
                f for f in os.listdir(self.work_dir) if f.endswith((".csv", ".xlsx"))
            ]
            logger.info(f"å·¥ä½œç›®å½•ä¸­çš„æ–‡ä»¶åˆ—è¡¨: {files}")

            for file in files:
                file_path = os.path.join(self.work_dir, file)
                if os.path.isfile(file_path):
                    try:
                        with open(file_path, "rb") as f:
                            content = f.read()
                            # ä½¿ç”¨å®˜æ–¹æ¨èçš„ files.write æ–¹æ³•
                            await self.sbx.files.write(f"/home/user/{file}", content)
                            logger.info(f"æˆåŠŸä¸Šä¼ æ–‡ä»¶åˆ°æ²™ç®±: {file}")
                    except Exception as e:
                        logger.error(f"ä¸Šä¼ æ–‡ä»¶ {file} å¤±è´¥: {str(e)}")
                        raise

        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ è¿‡ç¨‹å¤±è´¥: {str(e)}")
            raise

    async def _pre_execute_code(self):
        init_code = (
            "import matplotlib.pyplot as plt\n"
            # "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS']\n"
            # "plt.rcParams['axes.unicode_minus'] = False\n"
            # "plt.rcParams['font.family'] = 'sans-serif'\n"
        )
        await self.execute_code(init_code)

    async def execute_code(self, code: str) -> tuple[str, bool, str]:
        """æ‰§è¡Œä»£ç å¹¶è¿”å›ç»“æœ"""

        if not self.sbx:
            raise RuntimeError("æ²™ç®±ç¯å¢ƒæœªåˆå§‹åŒ–")

        logger.info(f"æ‰§è¡Œä»£ç : {code}")
        self.notebook_serializer.add_code_cell_to_notebook(code)

        text_to_gpt: list[str] = []
        content_to_display: list[OutputItem] | None = []
        error_occurred: bool = False
        error_message: str = ""

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="å¼€å§‹æ‰§è¡Œä»£ç "),
        )
        # æ‰§è¡Œ Python ä»£ç 
        logger.info("å¼€å§‹åœ¨æ²™ç®±ä¸­æ‰§è¡Œä»£ç ...")
        execution = await self.sbx.run_code(code)  # è¿”å› Execution å¯¹è±¡
        logger.info("ä»£ç æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="ä»£ç æ‰§è¡Œå®Œæˆ"),
        )

        # å¤„ç†æ‰§è¡Œé”™è¯¯
        if execution.error:
            error_occurred = True
            error_message = f"Error: {execution.error.name}: {execution.error.value}\n{execution.error.traceback}"
            error_message = self._truncate_text(error_message)
            logger.error(f"æ‰§è¡Œé”™è¯¯: {error_message}")
            text_to_gpt.append(self.delete_color_control_char(error_message))
            content_to_display.append(
                ErrorModel(
                    name=execution.error.name,
                    value=execution.error.value,
                    traceback=execution.error.traceback,
                )
            )
        # å¤„ç†æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯

        if execution.logs:
            if execution.logs.stdout:
                stdout_str = "\n".join(execution.logs.stdout)
                stdout_str = self._truncate_text(stdout_str)
                logger.info(f"æ ‡å‡†è¾“å‡º: {stdout_str}")
                text_to_gpt.append(stdout_str)
                content_to_display.append(
                    StdOutModel(msg="\n".join(execution.logs.stdout))
                )
                self.notebook_serializer.add_code_cell_output_to_notebook(stdout_str)

            if execution.logs.stderr:
                stderr_str = "\n".join(execution.logs.stderr)
                stderr_str = self._truncate_text(stderr_str)
                logger.warning(f"æ ‡å‡†é”™è¯¯: {stderr_str}")
                text_to_gpt.append(stderr_str)
                content_to_display.append(
                    StdErrModel(msg="\n".join(execution.logs.stderr))
                )

            # å¤„ç†æ‰§è¡Œç»“æœ
        if execution.results:
            for result in execution.results:
                # 1. æ–‡æœ¬æ ¼å¼
                if str(result):
                    content_to_display.append(
                        ResultModel(type="result", format="text", msg=str(result))
                    )
                # 2. HTMLæ ¼å¼
                if result._repr_html_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="html", msg=result._repr_html_()
                        )
                    )
                # 3. Markdownæ ¼å¼
                if result._repr_markdown_():
                    content_to_display.append(
                        ResultModel(
                            type="result",
                            format="markdown",
                            msg=result._repr_markdown_(),
                        )
                    )
                # 4. PNGå›¾ç‰‡ï¼ˆbase64å­—ç¬¦ä¸²ï¼Œå‰ç«¯å¯ç›´æ¥æ¸²æŸ“ï¼‰
                if result._repr_png_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="png", msg=result._repr_png_()
                        )
                    )
                # 5. JPEGå›¾ç‰‡
                if result._repr_jpeg_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="jpeg", msg=result._repr_jpeg_()
                        )
                    )
                # 6. SVG
                if result._repr_svg_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="svg", msg=result._repr_svg_()
                        )
                    )
                # 7. PDF
                if result._repr_pdf_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="pdf", msg=result._repr_pdf_()
                        )
                    )
                # 8. LaTeX
                if result._repr_latex_():
                    content_to_display.append(
                        ResultModel(
                            type="result", format="latex", msg=result._repr_latex_()
                        )
                    )
                # 9. JSON
                if result._repr_json_():
                    content_to_display.append(
                        ResultModel(
                            type="result",
                            format="json",
                            msg=json.dumps(result._repr_json_()),
                        )
                    )
                # 10. JavaScript
                if result._repr_javascript_():
                    content_to_display.append(
                        ResultModel(
                            type="result",
                            format="javascript",
                            msg=result._repr_javascript_(),
                        )
                    )

                    # å¤„ç†ä¸»è¦ç»“æœ
                # if result.is_main_result and result.text:
                #     result_text = self._truncate_text(result.text)
                #     logger.info(f"ä¸»è¦ç»“æœ: {result_text}")
                #     text_to_gpt.append(result_text)
                #     self.notebook_serializer.add_code_cell_output_to_notebook(
                #         result_text
                #     )

        # é™åˆ¶è¿”å›çš„æ–‡æœ¬æ€»é•¿åº¦

        for item in content_to_display:
            if isinstance(item, dict):
                if item.get("type") in ["stdout", "stderr", "error"]:
                    text_to_gpt.append(
                        self._truncate_text(
                            item.get("content") or item.get("value") or ""
                        )
                    )
            elif isinstance(item, ResultModel):
                if item.format in ["text", "html", "markdown", "json"]:
                    text_to_gpt.append(
                        self._truncate_text(f"[{item.format}]\n{item.msg}")
                    )
                elif item.format in ["png", "jpeg", "svg", "pdf"]:
                    text_to_gpt.append(
                        f"[{item.format} å›¾ç‰‡å·²ç”Ÿæˆï¼Œå†…å®¹ä¸º base64ï¼Œæœªå±•ç¤º]"
                    )

        logger.info(f"text_to_gpt: {text_to_gpt}")

        combined_text = "\n".join(text_to_gpt)

        # åœ¨ä»£ç æ‰§è¡Œå®Œæˆåï¼Œç«‹å³åŒæ­¥æ–‡ä»¶
        try:
            await self.download_all_files_from_sandbox()
            logger.info("æ–‡ä»¶åŒæ­¥å®Œæˆ")
        except Exception as e:
            logger.error(f"æ–‡ä»¶åŒæ­¥å¤±è´¥: {str(e)}")

        # ä¿å­˜åˆ°åˆ†æ®µå†…å®¹
        ## TODO: Base64 ç­‰å›¾åƒéœ€è¦ä¼˜åŒ–
        await self._push_to_websocket(content_to_display)

        return (
            combined_text,
            error_occurred,
            error_message,
        )

    async def get_created_images(self, section: str) -> list[str]:
        """è·å–å½“å‰ section åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨"""
        if not self.sbx:
            logger.warning("æ²™ç®±ç¯å¢ƒæœªåˆå§‹åŒ–")
            return []

        try:
            files = await self.sbx.files.list("./")
            for file in files:
                if file.path.endswith(".png") or file.path.endswith(".jpg"):
                    self.add_section(section)
                    self.section_output[section]["images"].append(file.name)

            self.created_images = list(
                set(self.section_output[section]["images"]) - set(self.created_images)
            )
            logger.info(f"{section}-è·å–åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨: {self.created_images}")
            return self.created_images
        except Exception as e:
            logger.error(f"è·å–åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    async def cleanup(self):
        """æ¸…ç†èµ„æºå¹¶å…³é—­æ²™ç®±"""
        try:
            if self.sbx:
                if await self.sbx.is_running():
                    try:
                        await self.download_all_files_from_sandbox()
                    except Exception as e:
                        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
                    finally:
                        await self.sbx.kill()
                        logger.info("æˆåŠŸå…³é—­æ²™ç®±ç¯å¢ƒ")
                else:
                    logger.warning("æ²™ç®±å·²ç»å…³é—­ï¼Œè·³è¿‡æ¸…ç†æ­¥éª¤")
        except Exception as e:
            logger.error(f"æ¸…ç†æ²™ç®±ç¯å¢ƒå¤±è´¥: {str(e)}")
            # è¿™é‡Œå¯ä»¥é€‰æ‹©ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™æ˜¯æ¸…ç†æ­¥éª¤

    async def download_all_files_from_sandbox(self) -> None:
        """ä»æ²™ç®±ä¸­ä¸‹è½½æ‰€æœ‰æ–‡ä»¶å¹¶ä¸æœ¬åœ°åŒæ­¥"""
        try:
            # è·å–æ²™ç®±ä¸­çš„æ–‡ä»¶åˆ—è¡¨
            sandbox_files = await self.sbx.files.list("/home/user")
            sandbox_files_dict = {f.name: f for f in sandbox_files}

            # è·å–æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
            local_files = set()
            if os.path.exists(self.work_dir):
                local_files = set(os.listdir(self.work_dir))

            # ä¸‹è½½æ–°æ–‡ä»¶æˆ–æ›´æ–°å·²ä¿®æ”¹çš„æ–‡ä»¶
            for file in sandbox_files:
                try:
                    # æ’é™¤ .bash_logoutã€.bashrc å’Œ .profile æ–‡ä»¶
                    if file.name in [".bash_logout", ".bashrc", ".profile"]:
                        continue

                    local_path = os.path.join(self.work_dir, file.name)
                    should_download = True

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°
                    if file.name in local_files:
                        # è¿™é‡Œå¯ä»¥æ·»åŠ æ–‡ä»¶ä¿®æ”¹æ—¶é—´æˆ–å†…å®¹å“ˆå¸Œçš„æ¯”è¾ƒ
                        # æš‚æ—¶ç®€å•å¤„ç†ï¼Œæœ‰åŒåæ–‡ä»¶å°±æ›´æ–°
                        pass

                    if should_download:
                        # ä½¿ç”¨ bytes æ ¼å¼è¯»å–æ–‡ä»¶å†…å®¹ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†äºŒè¿›åˆ¶æ•°æ®
                        content = await self.sbx.files.read(file.path, format="bytes")

                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(self.work_dir, exist_ok=True)

                        # å†™å…¥æ–‡ä»¶
                        with open(local_path, "wb") as f:
                            f.write(content)
                        logger.info(f"åŒæ­¥æ–‡ä»¶: {file.name}")

                except Exception as e:
                    logger.error(f"åŒæ­¥æ–‡ä»¶ {file.name} å¤±è´¥: {str(e)}")
                    continue

            logger.info("æ–‡ä»¶åŒæ­¥å®Œæˆ")

        except Exception as e:
            logger.error(f"æ–‡ä»¶åŒæ­¥å¤±è´¥: {str(e)}")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\interpreter_factory.py çš„å†…å®¹:
================================================================================
# interpreter_factory.py
from typing import Literal
from app.tools.e2b_interpreter import E2BCodeInterpreter
from app.tools.local_interpreter import LocalCodeInterpreter
from app.tools.notebook_serializer import NotebookSerializer
from app.config.setting import settings
from app.utils.log_util import logger


async def create_interpreter(
    kind: Literal["remote", "local"] = "local",
    *,
    task_id: str,
    work_dir: str,
    notebook_serializer: NotebookSerializer,
    timeout=3000,
):
    if not settings.E2B_API_KEY:
        logger.info("é»˜è®¤ä½¿ç”¨æœ¬åœ°è§£é‡Šå™¨")
        kind = "local"
    else:
        logger.info("ä½¿ç”¨è¿œç¨‹è§£é‡Šå™¨")
        kind = "remote"

    if kind == "remote":
        interp: E2BCodeInterpreter = await E2BCodeInterpreter.create(
            task_id=task_id,
            work_dir=work_dir,
            notebook_serializer=notebook_serializer,
        )
        await interp.initialize(timeout=timeout)
        return interp
    elif kind == "local":
        interp: LocalCodeInterpreter = LocalCodeInterpreter(
            task_id=task_id,
            work_dir=work_dir,
            notebook_serializer=notebook_serializer,
        )
        await interp.initialize()
        return interp
    else:
        raise ValueError(f"æœªçŸ¥ interpreter ç±»å‹ï¼š{kind}")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\local_interpreter.py çš„å†…å®¹:
================================================================================
from app.tools.base_interpreter import BaseCodeInterpreter
from app.tools.notebook_serializer import NotebookSerializer
import jupyter_client
from app.utils.log_util import logger
import os
from app.services.redis_manager import redis_manager
from app.schemas.response import (
    OutputItem,
    ResultModel,
    StdErrModel,
    SystemMessage,
)


class LocalCodeInterpreter(BaseCodeInterpreter):
    def __init__(
        self,
        task_id: str,
        work_dir: str,
        notebook_serializer: NotebookSerializer,
    ):
        super().__init__(task_id, work_dir, notebook_serializer)
        self.km, self.kc = None, None
        self.interrupt_signal = False

    async def initialize(self):
        # æœ¬åœ°å†…æ ¸ä¸€èˆ¬ä¸éœ€å¼‚æ­¥ä¸Šä¼ æ–‡ä»¶ï¼Œç›´æ¥åˆ‡æ¢ç›®å½•å³å¯
        # åˆå§‹åŒ– Jupyter å†…æ ¸ç®¡ç†å™¨å’Œå®¢æˆ·ç«¯
        logger.info("åˆå§‹åŒ–æœ¬åœ°å†…æ ¸")
        self.km, self.kc = jupyter_client.manager.start_new_kernel(
            kernel_name="python3"
        )
        self._pre_execute_code()

    def _pre_execute_code(self):
        init_code = (
            f"import os\n"
            f"work_dir = r'{self.work_dir}'\n"
            f"os.makedirs(work_dir, exist_ok=True)\n"
            f"os.chdir(work_dir)\n"
            f"print('å½“å‰å·¥ä½œç›®å½•:', os.getcwd())\n"
            f"import matplotlib.pyplot as plt\n"
            f"import matplotlib as mpl\n"
            # æ›´å®Œæ•´çš„ä¸­æ–‡å­—ä½“é…ç½®
            f"plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'PingFang SC', 'Hiragino Sans GB', 'Heiti SC', 'DejaVu Sans', 'sans-serif']\n"
            f"plt.rcParams['axes.unicode_minus'] = False\n"
            f"plt.rcParams['font.family'] = 'sans-serif'\n"
            f"mpl.rcParams['font.size'] = 12\n"
            f"mpl.rcParams['axes.labelsize'] = 12\n"
            f"mpl.rcParams['xtick.labelsize'] = 10\n"
            f"mpl.rcParams['ytick.labelsize'] = 10\n"
            # è®¾ç½®DPIä»¥è·å¾—æ›´æ¸…æ™°çš„æ˜¾ç¤º
        )
        self.execute_code_(init_code)

    async def execute_code(self, code: str) -> tuple[str, bool, str]:
        logger.info(f"æ‰§è¡Œä»£ç : {code}")
        #  æ·»åŠ ä»£ç åˆ°notebook
        self.notebook_serializer.add_code_cell_to_notebook(code)

        text_to_gpt: list[str] = []
        content_to_display: list[OutputItem] | None = []
        error_occurred: bool = False
        error_message: str = ""

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="å¼€å§‹æ‰§è¡Œä»£ç "),
        )
        # æ‰§è¡Œ Python ä»£ç 
        logger.info("å¼€å§‹åœ¨æœ¬åœ°æ‰§è¡Œä»£ç ...")
        execution = self.execute_code_(code)
        logger.info("ä»£ç æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")

        await redis_manager.publish_message(
            self.task_id,
            SystemMessage(content="ä»£ç æ‰§è¡Œå®Œæˆ"),
        )

        for mark, out_str in execution:
            if mark in ("stdout", "execute_result_text", "display_text"):
                text_to_gpt.append(self._truncate_text(f"[{mark}]\n{out_str}"))
                #  æ·»åŠ textåˆ°notebook
                content_to_display.append(
                    ResultModel(type="result", format="text", msg=out_str)
                )
                self.notebook_serializer.add_code_cell_output_to_notebook(out_str)

            elif mark in (
                "execute_result_png",
                "execute_result_jpeg",
                "display_png",
                "display_jpeg",
            ):
                # TODO: è§†è§‰æ¨¡å‹è§£é‡Šå›¾åƒ
                text_to_gpt.append(f"[{mark} å›¾ç‰‡å·²ç”Ÿæˆï¼Œå†…å®¹ä¸º base64ï¼Œæœªå±•ç¤º]")

                #  æ·»åŠ imageåˆ°notebook
                if "png" in mark:
                    self.notebook_serializer.add_image_to_notebook(out_str, "image/png")
                    content_to_display.append(
                        ResultModel(type="result", format="png", msg=out_str)
                    )
                else:
                    self.notebook_serializer.add_image_to_notebook(
                        out_str, "image/jpeg"
                    )
                    content_to_display.append(
                        ResultModel(type="result", format="jpeg", msg=out_str)
                    )

            elif mark == "error":
                error_occurred = True
                error_message = self.delete_color_control_char(out_str)
                error_message = self._truncate_text(error_message)
                logger.error(f"æ‰§è¡Œé”™è¯¯: {error_message}")
                text_to_gpt.append(error_message)
                #  æ·»åŠ erroråˆ°notebook
                self.notebook_serializer.add_code_cell_error_to_notebook(out_str)
                content_to_display.append(StdErrModel(msg=out_str))

        logger.info(f"text_to_gpt: {text_to_gpt}")
        combined_text = "\n".join(text_to_gpt)

        await self._push_to_websocket(content_to_display)

        return (
            combined_text,
            error_occurred,
            error_message,
        )

    def execute_code_(self, code) -> list[tuple[str, str]]:
        msg_id = self.kc.execute(code)
        logger.info(f"æ‰§è¡Œä»£ç : {code}")
        # Get the output of the code
        msg_list = []
        while True:
            try:
                iopub_msg = self.kc.get_iopub_msg(timeout=1)
                msg_list.append(iopub_msg)
                if (
                    iopub_msg["msg_type"] == "status"
                    and iopub_msg["content"].get("execution_state") == "idle"
                ):
                    break
            except:
                if self.interrupt_signal:
                    self.km.interrupt_kernel()
                    self.interrupt_signal = False
                continue

        all_output: list[tuple[str, str]] = []
        for iopub_msg in msg_list:
            if iopub_msg["msg_type"] == "stream":
                if iopub_msg["content"].get("name") == "stdout":
                    output = iopub_msg["content"]["text"]
                    all_output.append(("stdout", output))
            elif iopub_msg["msg_type"] == "execute_result":
                if "data" in iopub_msg["content"]:
                    if "text/plain" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["text/plain"]
                        all_output.append(("execute_result_text", output))
                    if "text/html" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["text/html"]
                        all_output.append(("execute_result_html", output))
                    if "image/png" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["image/png"]
                        all_output.append(("execute_result_png", output))
                    if "image/jpeg" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["image/jpeg"]
                        all_output.append(("execute_result_jpeg", output))
            elif iopub_msg["msg_type"] == "display_data":
                if "data" in iopub_msg["content"]:
                    if "text/plain" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["text/plain"]
                        all_output.append(("display_text", output))
                    if "text/html" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["text/html"]
                        all_output.append(("display_html", output))
                    if "image/png" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["image/png"]
                        all_output.append(("display_png", output))
                    if "image/jpeg" in iopub_msg["content"]["data"]:
                        output = iopub_msg["content"]["data"]["image/jpeg"]
                        all_output.append(("display_jpeg", output))
            elif iopub_msg["msg_type"] == "error":
                # TODO: æ­£ç¡®è¿”å›æ ¼å¼
                if "traceback" in iopub_msg["content"]:
                    output = "\n".join(iopub_msg["content"]["traceback"])
                    cleaned_output = self.delete_color_control_char(output)
                    all_output.append(("error", cleaned_output))
        return all_output

    async def get_created_images(self, section: str) -> list[str]:
        """è·å–æ–°åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨"""
        current_images = set()
        files = os.listdir(self.work_dir)
        for file in files:
            if file.endswith((".png", ".jpg", ".jpeg")):
                current_images.add(file)

        # è®¡ç®—æ–°å¢çš„å›¾ç‰‡
        new_images = current_images - self.last_created_images

        # æ›´æ–°last_created_imagesä¸ºå½“å‰çš„å›¾ç‰‡é›†åˆ
        self.last_created_images = current_images

        logger.info(f"æ–°åˆ›å»ºçš„å›¾ç‰‡åˆ—è¡¨: {new_images}")
        return list(new_images)  # æœ€åè½¬æ¢ä¸ºlistè¿”å›

    async def cleanup(self):
        # å…³é—­å†…æ ¸
        self.kc.shutdown()
        logger.info("å…³é—­å†…æ ¸")
        self.km.shutdown_kernel()

    def send_interrupt_signal(self):
        self.interrupt_signal = True

    def restart_jupyter_kernel(self):
        """Restart the Jupyter kernel and recreate the work directory."""
        self.kc.shutdown()
        self.km, self.kc = jupyter_client.manager.start_new_kernel(
            kernel_name="python3"
        )
        self.interrupt_signal = False
        self._create_work_dir()

    def _create_work_dir(self):
        """Ensure the working directory exists after a restart."""
        os.makedirs(self.work_dir, exist_ok=True)


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\notebook_serializer.py çš„å†…å®¹:
================================================================================
import nbformat
from nbformat import v4 as nbf
import ansi2html
import os


class NotebookSerializer:
    def __init__(self, work_dir=None, notebook_name="notebook.ipynb"):
        self.nb = nbf.new_notebook()
        self.notebook_path = None
        self.initialized = True
        self.segmentation_output_content = {}  # ä¿å­˜coder_agent åœ¨ jupyter ä¸­æ‰§è¡Œçš„ output ç»“æœå†…å®¹
        # {
        #     "eda": {
        #     }
        # }
        self.current_segmentation: str = ""

        self.init_notebook(work_dir, notebook_name)

    def init_notebook(self, work_dir=None, notebook_name="notebook.ipynb"):
        """åˆå§‹åŒ–notebookè·¯å¾„

        Args:
            work_dir (str): jupyterå·¥ä½œç›®å½•è·¯å¾„
            notebook_name (str): notebookæ–‡ä»¶å,é»˜è®¤ä¸ºnotebook.ipynb
        """
        if work_dir:
            # ç¡®ä¿ä½¿ç”¨jupyterå·¥ä½œç›®å½•
            base, ext = os.path.splitext(notebook_name)
            if ext.lower() != ".ipynb":
                notebook_name += ".ipynb"

            # åœ¨jupyterå·¥ä½œç›®å½•ä¸‹åˆ›å»ºnotebookæ–‡ä»¶
            self.notebook_path = os.path.join(work_dir, notebook_name)

            # if os.path.exists(self.notebook_path):
            #     raise FileExistsError(
            #         f"æ–‡ä»¶ {self.notebook_path} å·²å­˜åœ¨ã€‚è¯·é€‰æ‹©å…¶ä»–æ–‡ä»¶åã€‚"
            #     )

    def ansi_to_html(self, ansi_text):
        converter = ansi2html.Ansi2HTMLConverter()
        html_text = converter.convert(ansi_text)
        return html_text

    def write_to_notebook(self):
        if self.notebook_path:
            with open(self.notebook_path, "w", encoding="utf-8") as f:
                f.write(nbformat.writes(self.nb))

    def add_code_cell_to_notebook(self, code):
        code_cell = nbf.new_code_cell(source=code)
        self.nb["cells"].append(code_cell)
        self.write_to_notebook()

    def add_code_cell_output_to_notebook(self, output):
        """æ·»åŠ ä»£ç å•å…ƒæ ¼è¾“å‡º

        Args:
            output: ä»£ç è¾“å‡ºå†…å®¹
        """
        html_content = self.ansi_to_html(output)
        if self.current_segmentation:
            # ç¡®ä¿é”®å­˜åœ¨
            if self.current_segmentation not in self.segmentation_output_content:
                self.segmentation_output_content[self.current_segmentation] = ""
            self.segmentation_output_content[self.current_segmentation] += html_content

        cell_output = nbf.new_output(
            output_type="display_data", data={"text/html": html_content}
        )
        self.nb["cells"][-1]["outputs"].append(cell_output)
        self.write_to_notebook()

    def add_code_cell_error_to_notebook(self, error):
        nbf_error_output = nbf.new_output(
            output_type="error",
            ename="Error",
            evalue="Error message",
            traceback=[error],
        )
        self.nb["cells"][-1]["outputs"].append(nbf_error_output)
        self.write_to_notebook()

    def add_image_to_notebook(self, image, mime_type):
        image_output = nbf.new_output(
            output_type="display_data", data={mime_type: image}
        )
        self.nb["cells"][-1]["outputs"].append(image_output)
        self.write_to_notebook()

    def add_markdown_to_notebook(self, content, title=None):
        if title:
            content = "##### " + title + ":\n" + content
        markdown_cell = nbf.new_markdown_cell(content)
        self.nb["cells"].append(markdown_cell)
        self.write_to_notebook()

    def add_markdown_segmentation_to_notebook(self, content, segmentation):
        """æ·»åŠ markdownåˆ†æ®µå¹¶åˆå§‹åŒ–å¯¹åº”çš„outputå†…å®¹å­˜å‚¨

        Args:
            content: markdownå†…å®¹
            segmentation: åˆ†æ®µåç§°
        """
        self.current_segmentation = segmentation
        # åˆå§‹åŒ–è¯¥åˆ†æ®µçš„outputå†…å®¹
        self.segmentation_output_content[segmentation] = ""
        self.add_markdown_to_notebook(content, segmentation)

    def get_notebook_output_content(self, segmentation):
        return self.segmentation_output_content[segmentation]


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\openalex_scholar.py çš„å†…å®¹:
================================================================================
import requests
from typing import List, Dict, Any
from app.services.redis_manager import redis_manager
from app.schemas.response import ScholarMessage


class OpenAlexScholar:
    def __init__(self, task_id: str, email: str = None):
        """Initialize OpenAlex client.

        Args:
            email: Optional email for better API service
        """
        self.base_url = "https://api.openalex.org"
        self.email = email
        self.task_id = task_id

    def _get_request_url(self, endpoint: str) -> str:
        """Construct request URL with email parameter if provided."""
        if endpoint.startswith("/"):
            endpoint = endpoint[1:]
        return f"{self.base_url}/{endpoint}"

    def _get_abstract_from_index(self, abstract_inverted_index: Dict) -> str:
        """ä»abstract_inverted_indexä¸­é‡å»ºæ‘˜è¦æ–‡æœ¬

        Args:
            abstract_inverted_index: OpenAlex APIè¿”å›çš„å€’æ’ç´¢å¼•

        Returns:
            é‡å»ºçš„æ‘˜è¦æ–‡æœ¬
        """
        if not abstract_inverted_index:
            return ""

        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„ç©ºåˆ—è¡¨æ¥å­˜æ”¾æ‰€æœ‰å•è¯
        max_position = 0
        for positions in abstract_inverted_index.values():
            if positions and max(positions) > max_position:
                max_position = max(positions)

        words = [""] * (max_position + 1)

        # åœ¨æ­£ç¡®çš„ä½ç½®å¡«å…¥å•è¯
        for word, positions in abstract_inverted_index.items():
            for position in positions:
                words[position] = word

        # æ‹¼æ¥å•è¯å½¢æˆæ–‡æœ¬
        return " ".join(words).strip()

    async def search_papers(self, query: str, limit: int = 8) -> List[Dict[str, Any]]:
        """Search for papers using OpenAlex API.

        Args:
            query: Search query string
            limit: Maximum number of results to return

        Returns:
            List of papers with their details
        """
        # æ„å»ºåŸºç¡€ URL
        base_url = self._get_request_url("works")

        # è®¾ç½®è¯·æ±‚å‚æ•°ï¼Œæ ¹æ®APIæ”¯æŒçš„å­—æ®µè¿›è¡Œé€‰æ‹©
        params = {
            "search": query,
            "per_page": limit,
            "select": "id,title,display_name,authorships,cited_by_count,doi,publication_year,biblio,abstract_inverted_index",
        }

        # æ·»åŠ é‚®ç®±å‚æ•°åˆ°è¯·æ±‚URL
        if self.email:
            params["mailto"] = self.email
        else:
            raise ValueError("é…ç½®OpenAlexé‚®ç®±è·å–è®¿é—®æ–‡çŒ®æƒåˆ©")

        # è®¾ç½®è¯·æ±‚å¤´ï¼ŒåŒ…å«User-Agentå’Œé‚®ç®±ä¿¡æ¯
        headers = {
            "User-Agent": f"OpenAlexScholar/1.0 (mailto:{self.email})"
            if self.email
            else "OpenAlexScholar/1.0"
        }

        # è®© requests å¤„ç†å‚æ•°ç¼–ç å’Œ URL æ„å»º
        try:
            print(f"è¯·æ±‚ URL: {base_url} å‚æ•°: {params}")
            response = requests.get(base_url, params=params, headers=headers)
            print(f"å“åº”çŠ¶æ€: {response.status_code}")

            response.raise_for_status()
            results = response.json()
        except requests.exceptions.HTTPError as e:
            print(f"HTTP é”™è¯¯: {e}")
            if response.status_code == 403:
                print(
                    "æç¤º: 403é”™è¯¯é€šå¸¸æ„å‘³ç€æ‚¨éœ€è¦æä¾›æœ‰æ•ˆçš„é‚®ç®±åœ°å€æˆ–è€…éµå¾ªç¤¼è²Œæ± ï¼ˆpolite poolï¼‰è§„åˆ™"
                )
            if hasattr(response, "text"):
                print(f"å“åº”å†…å®¹: {response.text}")
            raise
        except Exception as e:
            print(f"è¯·æ±‚å‡ºé”™: {e}")
            raise

        papers = []
        paper_titles = []  # ç”¨äºå­˜å‚¨è®ºæ–‡æ ‡é¢˜
        for work in results.get("results", []):
            # ä»å€’æ’ç´¢å¼•ä¸­è·å–æ‘˜è¦
            abstract = self._get_abstract_from_index(
                work.get("abstract_inverted_index", {})
            )

            # è·å–ä½œè€…ä¿¡æ¯
            authors = []
            for authorship in work.get("authorships", []):
                author = authorship.get("author", {})
                if author:
                    author_info = {
                        "name": author.get("display_name"),
                        "position": authorship.get("author_position"),
                        "institution": authorship.get("institutions", [{}])[0].get(
                            "display_name"
                        )
                        if authorship.get("institutions")
                        else None,
                    }
                    authors.append(author_info)

            # è·å–å¼•ç”¨æ ¼å¼ä¿¡æ¯
            biblio = work.get("biblio", {})
            citation = {
                "volume": biblio.get("volume"),
                "issue": biblio.get("issue"),
                "first_page": biblio.get("first_page"),
                "last_page": biblio.get("last_page"),
            }

            paper = {
                "title": work.get("display_name") or work.get("title", ""),
                "abstract": abstract,
                "authors": authors,
                "citations_count": work.get("cited_by_count"),
                "doi": work.get("doi"),
                "publication_year": work.get("publication_year"),
                "citation_info": citation,
                # æ„å»ºå¼•ç”¨æ ¼å¼
                "citation_format": self._format_citation(work),
            }
            papers.append(paper)
            paper_titles.append(paper["title"])  # æ·»åŠ æ ‡é¢˜åˆ°åˆ—è¡¨

        await redis_manager.publish_message(
            self.task_id,
            ScholarMessage(
                input={"query": query},
                output=paper_titles,  # åªå‘é€è®ºæ–‡æ ‡é¢˜åˆ—è¡¨
            ),
        )

        return papers

    def papers_to_str(self, papers: List[Dict[str, Any]]) -> str:
        """å°†æ–‡çŒ®åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
        result = ""
        for paper in papers:
            result += "\n" + "=" * 80
            result += f"\næ ‡é¢˜: {paper['title']}"
            result += f"\næ‘˜è¦: {paper['abstract']}"
            result += "\nä½œè€…:"
            for author in paper["authors"]:
                result += f"- {author['name']}"
            result += f"\nå¼•ç”¨æ¬¡æ•°: {paper['citations_count']}"
            result += f"\nå‘è¡¨å¹´ä»½: {paper['publication_year']}"
            result += f"\nå¼•ç”¨æ ¼å¼:\n{paper['citation_format']}"
            result += "=" * 80
        return result

    def _format_citation(self, work: Dict[str, Any]) -> str:
        """Format citation in a readable format."""
        # è·å–æ‰€æœ‰ä½œè€…
        authors = [
            authorship.get("author", {}).get("display_name")
            for authorship in work.get("authorships", [])
            if authorship.get("author")
        ]

        # æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨
        if len(authors) > 3:
            authors_str = f"{authors[0]} et al."
        else:
            authors_str = ", ".join(authors)

        # è·å–æ ‡é¢˜
        title = work.get("display_name") or work.get("title", "")

        # è·å–å¹´ä»½
        year = work.get("publication_year", "")

        # è·å–DOI
        doi = work.get("doi", "")

        # æ„å»ºå¼•ç”¨æ ¼å¼
        citation = f"{authors_str} ({year}). {title}."
        if doi:
            citation += f" DOI: {doi}"

        return citation


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\tools\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\cli.py çš„å†…å®¹:
================================================================================
from textwrap import dedent

def center_cli_str(text: str, width: int | None = None):
    import shutil

    width = width or shutil.get_terminal_size().columns
    lines = text.split("\n")
    max_line_len = max(len(line) for line in lines)
    return "\n".join(
        (line + " " * (max_line_len - len(line))).center(width) for line in lines
    )


def get_ascii_banner(center: bool = True) -> str:
    text = dedent(
        r"""
        ===============================================================================
         __  __       _   _     __  __           _      _                          _   
        |  \/  |     | | | |   |  \/  |         | |    | |   /\                   | |  
        | \  / | __ _| |_| |__ | \  / | ___   __| | ___| |  /  \   __ _  ___ _ __ | |_ 
        | |\/| |/ _` | __| '_ \| |\/| |/ _ \ / _` |/ _ \ | / /\ \ / _` |/ _ \ '_ \| __|
        | |  | | (_| | |_| | | | |  | | (_) | (_| |  __/ |/ ____ \ (_| |  __/ | | | |_ 
        |_|  |_|\__,_|\__|_| |_|_|  |_|\___/ \__,_|\___|_/_/    \_\__, |\___|_| |_|\__|
                                                                    __/ |               
                                                                |___/                
        ===============================================================================
        """,
    ).strip()
    if center:
        return center_cli_str(text)
    else:
        return text


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\common_utils.py çš„å†…å®¹:
================================================================================
import os
import datetime
import hashlib
import tomllib
from app.schemas.enums import CompTemplate
from app.utils.log_util import logger
import re
import pypandoc
from app.config.setting import settings
from icecream import ic


def create_task_id() -> str:
    """ç”Ÿæˆä»»åŠ¡ID"""
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œéšæœºhash
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    random_hash = hashlib.md5(str(datetime.datetime.now()).encode()).hexdigest()[:8]
    return f"{timestamp}-{random_hash}"


def create_work_dir(task_id: str) -> str:
    # è®¾ç½®ä¸»å·¥ä½œç›®å½•å’Œå­ç›®å½•
    work_dir = os.path.join("project", "work_dir", task_id)

    try:
        # åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨ä¹Ÿä¸ä¼šæŠ¥é”™
        os.makedirs(work_dir, exist_ok=True)
        return work_dir
    except Exception as e:
        # æ•è·å¹¶è®°å½•åˆ›å»ºç›®å½•æ—¶çš„å¼‚å¸¸
        logger.error(f"åˆ›å»ºå·¥ä½œç›®å½•å¤±è´¥: {str(e)}")
        raise


def get_work_dir(task_id: str) -> str:
    work_dir = os.path.join("project", "work_dir", task_id)
    if os.path.exists(work_dir):
        return work_dir
    else:
        logger.error(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {work_dir}")
        raise FileNotFoundError(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {work_dir}")


#  TODO: æ˜¯ä¸æ˜¯åº”è¯¥å°† Prompt å†™æˆä¸€ä¸ª class
def get_config_template(comp_template: CompTemplate = CompTemplate.CHINA) -> dict:
    if comp_template == CompTemplate.CHINA:
        return load_toml(os.path.join("app", "config", "md_template.toml"))


def load_toml(path: str) -> dict:
    with open(path, "rb") as f:
        return tomllib.load(f)


def load_markdown(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def get_current_files(folder_path: str, type: str = "all") -> list[str]:
    files = os.listdir(folder_path)
    if type == "all":
        return files
    elif type == "md":
        return [file for file in files if file.endswith(".md")]
    elif type == "ipynb":
        return [file for file in files if file.endswith(".ipynb")]
    elif type == "data":
        return [
            file for file in files if file.endswith(".xlsx") or file.endswith(".csv")
        ]
    elif type == "image":
        return [
            file for file in files if file.endswith(".png") or file.endswith(".jpg")
        ]


# åˆ¤æ–­contentæ˜¯å¦åŒ…å«å›¾ç‰‡ xx.png,å¯¹å…¶å¤„ç†ä¸º    ![filename](http://localhost:8000/static/20250428-200915-ebc154d4/filename.jpg)
def transform_link(task_id: str, content: str):
    content = re.sub(
        r"!\[(.*?)\]\((.*?\.(?:png|jpg|jpeg|gif|bmp|webp))\)",
        lambda match: f"![{match.group(1)}]({settings.SERVER_HOST}/static/{task_id}/{match.group(2)})",
        content,
    )
    return content


# TODO: fix å…¬å¼æ˜¾ç¤º
def md_2_docx(task_id: str):
    work_dir = get_work_dir(task_id)
    md_path = os.path.join(work_dir, "res.md")
    docx_path = os.path.join(work_dir, "res.docx")

    extra_args = [
        "--resource-path",
        str(work_dir),
        "--mathml",  # MathML æ ¼å¼å…¬å¼
        "--standalone",
    ]

    pypandoc.convert_file(
        source_file=md_path,
        to="docx",
        outputfile=docx_path,
        format="markdown+tex_math_dollars",
        extra_args=extra_args,
    )
    print(f"è½¬æ¢å®Œæˆ: {docx_path}")
    logger.info(f"è½¬æ¢å®Œæˆ: {docx_path}")


def split_footnotes(text: str) -> tuple[str, list[tuple[str, str]]]:
    main_text = re.sub(
        r"\n\[\^\d+\]:.*?(?=\n\[\^|\n\n|\Z)", "", text, flags=re.DOTALL
    ).strip()

    # åŒ¹é…è„šæ³¨å®šä¹‰
    footnotes = re.findall(r"\[\^(\d+)\]:\s*(.+?)(?=\n\[\^|\n\n|\Z)", text, re.DOTALL)
    logger.info(f"main_text:{main_text} \n footnotes:{footnotes}")
    return main_text, footnotes


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\data_recorder.py çš„å†…å®¹:
================================================================================
import json
import os
from app.utils.log_util import logger
from typing import Any, Dict


# TODO: è®°å½•æ•°æ®
# data analysis : save all data and result
# agent-histroy, token usgae, , cost , workflow cost , res
class DataRecorder:
    def __init__(self, log_work_dir: str = ""):
        self.total_cost = 0.0
        self.agents_chat_history = {}
        # {"agent_name": [{}, {}, ...]
        #
        # }
        self.chat_completion = {}
        # {"agent_name": [ChatCompletion, ChatCompletion, ...]
        #
        # }
        self.log_work_dir = log_work_dir
        self.token_usage = {}

        self.initialized = True

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        logger.info("\n=== Token Usage and Cost Summary ===")

        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        headers = ["Agent", "Chats", "Prompt", "Completion", "Total", "Cost ($)"]
        rows = []

        for agent_name, usage in self.token_usage.items():
            rows.append(
                [
                    agent_name,
                    usage["chat_count"],
                    usage["prompt_tokens"],
                    usage["completion_tokens"],
                    usage["total_tokens"],
                    f"{usage['cost']:.4f}",
                ]
            )

        # æ·»åŠ æ€»è®¡è¡Œ
        total_chats = sum(usage["chat_count"] for usage in self.token_usage.values())
        total_prompt = sum(
            usage["prompt_tokens"] for usage in self.token_usage.values()
        )
        total_completion = sum(
            usage["completion_tokens"] for usage in self.token_usage.values()
        )
        total_tokens = sum(usage["total_tokens"] for usage in self.token_usage.values())

        rows.append(
            [
                "TOTAL",
                total_chats,
                total_prompt,
                total_completion,
                total_tokens,
                f"{self.total_cost:.4f}",
            ]
        )

        # ä½¿ç”¨ RichPrinter æ‰“å°è¡¨æ ¼
        from utils.RichPrinter import RichPrinter

        RichPrinter.table(
            headers=headers,
            rows=rows,
            title="Token Usage and Cost Summary",
            column_styles=["cyan", "magenta", "blue", "blue", "blue", "green"],
        )

    def write_to_json(self, to_save: dict, file_name: str):
        if self.log_work_dir:
            json_path = os.path.join(self.log_work_dir, file_name)
            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(to_save, f, ensure_ascii=False, indent=4)
            except Exception as e:
                logger.error(f"å†™å…¥jsonæ–‡ä»¶å¤±è´¥: {e}")

    def append_chat_history(self, msg: dict, agent_name: str) -> None:
        """æ·»åŠ èŠå¤©å†å²è®°å½•"""
        if agent_name not in self.agents_chat_history:
            self.agents_chat_history[agent_name] = []
        self.agents_chat_history[agent_name].append(msg)
        self.write_to_json(self.agents_chat_history, "chat_history.json")

    def chat_completion_to_dict(self, completion: Any) -> Dict:
        """å°† ChatCompletion å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        return {
            "id": completion.id,
            "choices": [
                {
                    "index": choice.index,
                    "message": {
                        "role": choice.message.role,
                        "content": choice.message.content,
                        "tool_calls": [
                            {
                                "id": tool_call.id,
                                "type": tool_call.type,
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments,
                                },
                            }
                            for tool_call in (choice.message.tool_calls or [])
                        ]
                        if hasattr(choice.message, "tool_calls")
                        else None,
                    },
                    "finish_reason": choice.finish_reason,
                }
                for choice in completion.choices
            ],
            "created": completion.created,
            "model": completion.model,
            "usage": {
                "completion_tokens": completion.usage.completion_tokens,
                "prompt_tokens": completion.usage.prompt_tokens,
                "total_tokens": completion.usage.total_tokens,
            }
            if hasattr(completion, "usage")
            else None,
            "system_fingerprint": completion.system_fingerprint
            if hasattr(completion, "system_fingerprint")
            else None,
        }

    def append_chat_completion(self, completion: Any, agent_name: str) -> None:
        """æ·»åŠ èŠå¤©å®Œæˆè®°å½•"""
        if agent_name not in self.chat_completion:
            self.chat_completion[agent_name] = []

        # å°† ChatCompletion å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        completion_dict = self.chat_completion_to_dict(completion)
        self.chat_completion[agent_name].append(completion_dict)

        # æ›´æ–° token ä½¿ç”¨ç»Ÿè®¡
        self.update_token_usage(completion, agent_name)

        # å†™å…¥ JSON æ–‡ä»¶
        self.write_to_json(self.chat_completion, "chat_completion.json")

    def update_token_usage(self, completion: Any, agent_name: str) -> None:
        """æ›´æ–° token ä½¿ç”¨ç»Ÿè®¡å’Œè´¹ç”¨
        Args:
            completion: ChatCompletion å¯¹è±¡
            agent_name: ä»£ç†åç§°
        """
        if not hasattr(completion, "usage"):
            return

        if agent_name not in self.token_usage:
            self.token_usage[agent_name] = {
                "completion_tokens": 0,
                "prompt_tokens": 0,
                "total_tokens": 0,
                "chat_count": 0,
                "cost": 0.0,  # æ·»åŠ è´¹ç”¨å­—æ®µ
            }

        usage = completion.usage
        model = completion.model

        # æ›´æ–° token ç»Ÿè®¡
        self.token_usage[agent_name]["completion_tokens"] += usage.completion_tokens
        self.token_usage[agent_name]["prompt_tokens"] += usage.prompt_tokens
        self.token_usage[agent_name]["total_tokens"] += usage.total_tokens
        self.token_usage[agent_name]["chat_count"] += 1

        # è®¡ç®—æœ¬æ¬¡è¯·æ±‚çš„è´¹ç”¨
        cost = self.calculate_cost(model, usage.prompt_tokens, usage.completion_tokens)
        self.token_usage[agent_name]["cost"] += cost
        self.total_cost += cost  # æ›´æ–°æ€»è´¹ç”¨

        # å†™å…¥ JSON æ–‡ä»¶
        self.write_to_json(self.token_usage, "token_usage.json")

    def calculate_cost(
        self, model: str, prompt_tokens: int, completion_tokens: int
    ) -> float:
        """è®¡ç®—APIè°ƒç”¨è´¹ç”¨
        Args:
            model: æ¨¡å‹åç§°
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
        Returns:
            float: è´¹ç”¨ï¼ˆrmbï¼‰
        """
        # å®šä¹‰æ¨¡å‹ä»·æ ¼ï¼ˆæ¯1000ä¸ªtokençš„ä»·æ ¼ï¼Œå•ä½ï¼šrmbï¼‰
        model_prices = {
            "gpt-4-turbo-preview": {"prompt": 0.01, "completion": 0.03},
            "gpt-4": {"prompt": 0.03, "completion": 0.06},
            "gpt-3.5-turbo": {"prompt": 0.0005, "completion": 0.0015},
            "qwen-max-latest": {"prompt": 0.0024, "completion": 0.0096},  # ç¤ºä¾‹ä»·æ ¼
        }

        # è·å–æ¨¡å‹ä»·æ ¼ï¼Œå¦‚æœæ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­ä½¿ç”¨é»˜è®¤ä»·æ ¼
        model_price = model_prices.get(
            model,
            {"prompt": 0.0001, "completion": 0.0001},  # é»˜è®¤ä»·æ ¼
        )

        # è®¡ç®—è´¹ç”¨ï¼ˆå°†tokenæ•°è½¬æ¢ä¸ºåƒåˆ†æ¯”ï¼‰
        prompt_cost = (prompt_tokens / 1000.0) * model_price["prompt"]
        completion_cost = (completion_tokens / 1000.0) * model_price["completion"]

        return prompt_cost + completion_cost


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\log_util.py çš„å†…å®¹:
================================================================================
import os
import sys
import time
from loguru import logger as _logger


class LoggerInitializer:
    def __init__(self):
        self.log_path = os.path.join(os.getcwd(), "logs")
        self.__ensure_log_directory_exists()
        self.log_path_error = os.path.join(
            self.log_path, f"{time.strftime('%Y-%m-%d')}_error.log"
        )

    def __ensure_log_directory_exists(self):
        """
        ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        """
        if not os.path.exists(self.log_path):
            os.mkdir(self.log_path)

    @staticmethod
    def __filter(log: dict):
        """
        è‡ªå®šä¹‰æ—¥å¿—è¿‡æ»¤å™¨ï¼Œæ·»åŠ trace_id
        """
        return log

    def init_log(self):
        """
        åˆå§‹åŒ–æ—¥å¿—é…ç½®
        """
        # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼
        format_str = (
            "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
            "<level>{message}</level>"
        )
        _logger.remove()
        # ç§»é™¤åé‡æ–°æ·»åŠ sys.stderr, ç›®çš„: æ§åˆ¶å°è¾“å‡ºä¸æ–‡ä»¶æ—¥å¿—å†…å®¹å’Œç»“æ„ä¸€è‡´
        _logger.add(sys.stderr, filter=self.__filter, format=format_str, enqueue=True)
        _logger.add(
            self.log_path_error,
            filter=self.__filter,
            format=format_str,
            rotation="50MB",
            encoding="utf-8",
            enqueue=True,
            compression="zip",
        )

        return _logger


# åˆå§‹åŒ–æ—¥å¿—å¤„ç†å™¨
log_initializer = LoggerInitializer()
logger = log_initializer.init_log()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\RichPrinter.py çš„å†…å®¹:
================================================================================
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from typing import Optional, List, Any, Dict
from rich import print as rprint
from app.utils.log_util import logger


class RichPrinter:
    # ç±»å±æ€§ï¼šå…¨å±€æ ·å¼é…ç½®
    _styles = {
        "success": {"emoji": "âœ…", "color": "green", "prefix": "æˆåŠŸ"},
        "error": {"emoji": "âŒ", "color": "red", "prefix": "é”™è¯¯"},
        "warning": {"emoji": "âš ï¸", "color": "yellow", "prefix": "è­¦å‘Š"},
        "info": {"emoji": "â„¹ï¸", "color": "blue", "prefix": "ä¿¡æ¯"},
        "debug": {"emoji": "ğŸ", "color": "magenta", "prefix": "è°ƒè¯•"},
    }

    # å…±äº«çš„ Console å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    _console = Console()

    @classmethod
    def _format_message(
        cls,
        message: str,
        style_type: str,
        color: Optional[str] = None,
        emoji: Optional[str] = None,
        prefix: Optional[str] = None,
    ) -> Text:
        """æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºç»Ÿä¸€æ ·å¼"""
        style = cls._styles.get(style_type, {})
        emoji = emoji or style.get("emoji", "")
        color = color or style.get("color", "white")
        prefix = prefix or style.get("prefix", "")

        formatted = Text()
        if emoji:
            formatted.append(f"{emoji} ", style="bold")
        if prefix:
            formatted.append(f"{prefix}: ", style=f"bold {color}")
        formatted.append(message, style=color)
        return formatted

    @classmethod
    def success(cls, message: str, **kwargs):
        cls._print_panel(message, style_type="success", **kwargs)

    @classmethod
    def error(cls, message: str, **kwargs):
        cls._print_panel(message, style_type="error", **kwargs)

    @classmethod
    def warning(cls, message: str, **kwargs):
        cls._print_panel(message, style_type="warning", **kwargs)

    @staticmethod
    def print_agent_msg(message: str, agent_name: str):
        logger.info(f"{agent_name}: {message}")
        if agent_name == "CoderAgent":
            rprint(
                f"[bold purple on green]{agent_name}[/bold purple on green]: {message}"
            )
        elif agent_name == "WriterAgent":
            rprint(
                f"[bold purple on yellow]{agent_name}[/bold purple on yellow]: {message}"
            )
        elif agent_name == "test_agent":
            rprint(f"[bold white on blue]{agent_name}[/bold white on blue]: {message}")
        else:
            rprint(f"[bold white]{agent_name}[/bold white]: {message}")

    @classmethod
    def _print_panel(
        cls,
        message: str,
        style_type: str,
        title: Optional[str] = None,
        color: Optional[str] = None,
        emoji: Optional[str] = None,
        prefix: Optional[str] = None,
        panel_kwargs: Optional[Dict] = None,
    ):
        """é€šç”¨å¸¦é¢æ¿æ ·å¼çš„æ‰“å°æ–¹æ³•"""
        text = cls._format_message(message, style_type, color, emoji, prefix)
        default_panel_args = {
            "title": title or style_type.upper(),
            "border_style": color or cls._styles[style_type]["color"],
            "padding": (1, 4),
        }
        panel_args = {**default_panel_args, **(panel_kwargs or {})}
        cls._console.print(Panel.fit(text, **panel_args))

    @classmethod
    def table(
        cls,
        headers: List[str],
        rows: List[List[Any]],
        title: str = "æ•°æ®è¡¨æ ¼",
        column_styles: Optional[List[str]] = None,
    ):
        """å¿«é€Ÿæ‰“å°è¡¨æ ¼"""
        table = Table(title=title, show_header=True, header_style="bold cyan")
        column_styles = column_styles or ["magenta"] * len(headers)

        for header, style in zip(headers, column_styles):
            table.add_column(header, style=style)

        for row in rows:
            table.add_row(*[str(item) for item in row])

        cls._console.print(table)

    @classmethod
    def workflow_start(cls):
        """æ‰“å°å·¥ä½œæµå¼€å§‹ä¿¡æ¯"""
        cls._console.print()  # æ·»åŠ å‰ç½®æ¢è¡Œ
        formatted = Text()
        formatted.append("ğŸš€ ", style="bold")
        formatted.append("å¼€å§‹æ‰§è¡Œå·¥ä½œæµ", style="bold blue")
        cls._console.print(Panel.fit(formatted, border_style="blue", padding=(1, 4)))
        logger.info("\n=======================å¼€å§‹æ‰§è¡Œå·¥ä½œæµ=======================\n")

    @classmethod
    def workflow_end(cls):
        """æ‰“å°å·¥ä½œæµç»“æŸä¿¡æ¯"""
        cls._console.print()  # æ·»åŠ å‰ç½®æ¢è¡Œ
        formatted = Text()
        formatted.append("âœ¨ ", style="bold")
        formatted.append("å·¥ä½œæµæ‰§è¡Œå®Œæˆ", style="bold green")
        cls._console.print(Panel.fit(formatted, border_style="green", padding=(1, 4)))
        logger.info("\n=======================å·¥ä½œæµæ‰§è¡Œå®Œæˆ=======================\n")

    @classmethod
    def agent_start(cls, agent_name: str):
        """æ‰“å° Agent å¼€å§‹ä¿¡æ¯"""
        cls._console.print()  # æ·»åŠ å‰ç½®æ¢è¡Œ
        formatted = Text()
        formatted.append("ğŸ¤– ", style="bold")
        formatted.append(f"Agent: {agent_name} ", style="bold cyan")
        formatted.append("å¼€å§‹æ‰§è¡Œ", style="bold blue")
        cls._console.print(Panel.fit(formatted, border_style="blue", padding=(1, 4)))
        logger.info(f"\n================Agent: {agent_name}å¼€å§‹=================\n")

    @classmethod
    def agent_end(cls, agent_name: str):
        """æ‰“å° Agent ç»“æŸä¿¡æ¯"""
        cls._console.print()  # æ·»åŠ å‰ç½®æ¢è¡Œ
        formatted = Text()
        formatted.append("âœ¨ ", style="bold")
        formatted.append(f"Agent: {agent_name} ", style="bold cyan")
        formatted.append("æ‰§è¡Œå®Œæˆ", style="bold green")
        cls._console.print(Panel.fit(formatted, border_style="green", padding=(1, 4)))
        logger.info(f"\n================Agent: {agent_name}ç»“æŸ==================\n")


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\track.py çš„å†…å®¹:
================================================================================
from litellm.integrations.custom_logger import CustomLogger
import litellm


class AgentMetrics(CustomLogger):
    #### ASYNC ####

    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):
        try:
            # response_cost = kwargs.get("response_cost", 0)
            # print("streaming response_cost", response_cost)
            print("agent_name", kwargs["litellm_params"]["metadata"]["agent_name"])
        except:
            pass

    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):
        print(f"On Async Failure")


# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
agent_metrics = AgentMetrics()


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\app\utils\__init__.py çš„å†…å®¹:
================================================================================


================================================================================
E:\repo1\MCM\mcmaa\math_model_agent\project\work_dir\.gitkeep çš„å†…å®¹:
================================================================================


